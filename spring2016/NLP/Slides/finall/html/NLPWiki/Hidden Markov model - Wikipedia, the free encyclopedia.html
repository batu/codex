<!DOCTYPE html>
<!-- saved from url=(0049)https://en.wikipedia.org/wiki/Hidden_Markov_model -->
<html lang="en" dir="ltr" class="client-js ve-not-available"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Hidden Markov model - Wikipedia, the free encyclopedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Hidden_Markov_model","wgTitle":"Hidden Markov model","wgCurRevisionId":716209275,"wgRevisionId":716209275,"wgArticleId":98770,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Commons category with local link same as on Wikidata","Wikipedia articles with GND identifiers","Bioinformatics","Hidden Markov models","Markov models","Articles with example Python code"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Hidden_Markov_model","wgRelevantArticleId":98770,"wgRequestId":"VzicZQpAMFwAAJlNj4YAAABE","wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","usePageImages":true,"usePageDescriptions":true},"wgPreferredVariant":"en","wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSAcceptLanguageList":["fr","fr-fr","en-us","en"],"wgULSCurrentAutonym":"English","wgFlaggedRevsParams":{"tags":{"status":{"levels":1,"quality":2,"pristine":3}}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgNoticeProject":"wikipedia","wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCentralAuthMobileDomain":false,"wgWikibaseItemId":"Q176769","wgVisualEditorToolbarScrollOffset":0});mw.loader.implement("user.options",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["mediawiki.page.startup","mediawiki.legacy.wikibits","ext.centralauth.centralautologin","mmv.head","ext.visualEditor.desktopArticleTarget.init","ext.uls.init","ext.uls.interface","ext.quicksurveys.init","mw.MediaWikiPlayer.loader","mw.PopUpMediaTransform","ext.centralNotice.bannerController","skins.vector.js"]);});</script>
<link rel="stylesheet" href="./Hidden Markov model - Wikipedia, the free encyclopedia_files/load.php">
<style>
@-webkit-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-webkit-transform:translateY(-20px)}100%{opacity:1;-webkit-transform:translateY(0)}}@-moz-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-moz-transform:translateY(-20px)}100%{opacity:1;-moz-transform:translateY(0)}}@-o-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-o-transform:translateY(-20px)}100%{opacity:1;-o-transform:translateY(0)}}@keyframes centralAuthPPersonalAnimation{0%{opacity:0;transform:translateY(-20px)}100%{opacity:1;transform:translateY(0)}}.centralAuthPPersonalAnimation{-webkit-animation-duration:1s;-moz-animation-duration:1s;-o-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;-moz-animation-fill-mode:both;-o-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:centralAuthPPersonalAnimation;-moz-animation-name:centralAuthPPersonalAnimation;-o-animation-name:centralAuthPPersonalAnimation;animation-name:centralAuthPPersonalAnimation}
.postedit-container{margin:0 auto;position:fixed;top:0;height:0;left:50%;z-index:1000;font-size:13px}.postedit-container:hover{cursor:pointer}.postedit{position:relative;top:0.6em;left:-50%;padding:.6em 3.6em .6em 1.1em;line-height:1.5625em;color:#626465;background-color:#f4f4f4;border:1px solid #dcd9d9;text-shadow:0 0.0625em 0 rgba(255,255,255,0.5);border-radius:5px;box-shadow:0 2px 5px 0 #ccc;-webkit-transition:all 0.25s ease-in-out;-moz-transition:all 0.25s ease-in-out;-ms-transition:all 0.25s ease-in-out;-o-transition:all 0.25s ease-in-out;transition:all 0.25s ease-in-out}.skin-monobook .postedit{top:6em !important}.postedit-faded{opacity:0}.postedit-icon{padding-left:41px;  line-height:25px;background-repeat:no-repeat;background-position:8px 50%}.postedit-icon-checkmark{background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAB9ElEQVR4AZWRA3AYURQArxrVHtW2bdu2bdu2zdi2bdu2bWxs7zeehZaw4f70kbs+zI3e/nWK+RWx3aOFlrL56Sy5SxrruG69hlv6OyK+mz+8KDSXdXembj0ispT7tjs4ZTIbpYBvxGSGKzZTeFrb7W/meN002swFs0U8ttpHTkF2BvCqWQrW35929bTsKm5Zb+SEwWwcY8wAngB9m7Z+d+rIPZ/npdy12M5p47n8dXsCYAf0qPy06eGMdktuDu9Qf+JmKl3SWM91qzVcN9tAbEYkwMaq0tyb1m/To5kP170el/BK8/qa6sJr70ydf+T/Uu5ab+Oo/lS0AkUBpIFWlZ9WPhxpse/PHO7YbOOczjL0vZV2lNxPPtG73dYXM+xvm2znrOl83tidoqCwMBgYXsPFB0on5S6pr+eK5TKuW67lgvaKvF8mL1dtfTL32FHxRdyx3cQpg7m4x9sCXKkTIzA4LDH44zWdzaUf71hv5rTG4uyzcusybxSX7aThbMQ8XgCYAp3rzTTQOiIh9PNlzY3FSuZxrzjme1Y7uGS6kjsWO4jPjM4FVjRZsvD4kO9XtTZzQn82NyzWc0B7AmZh6gA/hOYSGhfw9YbOVnarj+S7800AL2BIsxUAbWNToj7bhBuQmZcOsFdoKUC74rGheCwXmqAIQTc9jQcrADIAAAAASUVORK5CYII=);background-image:url(/w/resources/src/mediawiki.action/images/green-checkmark.png?d94f1)!ie;background-position:left}.postedit-close{position:absolute;padding:0 .8em;right:0;top:0;font-size:1.25em;font-weight:bold;line-height:2.3em;color:black;text-shadow:0 0.0625em 0 white;text-decoration:none;opacity:0.2;filter:alpha(opacity=20)}.postedit-close:hover{color:black;text-decoration:none;opacity:0.4;filter:alpha(opacity=40)}
.uls-menu a{cursor:pointer}.uls-menu.callout .caret-before{border-top:20px solid transparent;border-right:20px solid #C9C9C9;border-bottom:20px solid transparent;display:inline-block;left:-21px;top:30px;position:absolute}.uls-menu.callout .caret-after{border-top:20px solid transparent;border-right:20px solid #FCFCFC;border-bottom:20px solid transparent;display:inline-block;left:-20px;top:30px;position:absolute}.uls-ui-languages button{width:22%;text-overflow:ellipsis;margin-right:2%;white-space:nowrap;overflow:hidden;padding:0.5em 0}button.uls-more-languages{width:auto}.settings-title{font-size:11pt}.settings-text{color:#555555;font-size:9pt}div.display-settings-block:hover .settings-text{color:#252525}
.ext-quick-survey-panel,.ext-qs-loader-bar{width:auto;background-color:#eeeeee} .ext-qs-loader-bar{display:block;height:100px;margin-left:1.4em;clear:right;float:right;background-color:#eeeeee}.ext-qs-loader-bar.mw-ajax-loader{top:0}@media all and (min-width:720px){.ext-qs-loader-bar,.ext-quick-survey-panel{margin-left:1.4em;width:300px;clear:right;float:right}}
#container{position:relative;min-height:100%}#container,video{width:100%;height:100%}#playerContainer{overflow:hidden;position:relative;height:100%;background:#000}#videoHolder{position:relative;overflow:hidden}.fullscreen #playerContainer{position:absolute !important;width:100% !important;height:100%! important;z-index:9999;min-height:100%;top:0;left:0;margin:0}.mwEmbedPlayer{width:100%;height:100%;overflow:hidden;position:absolute;top:0;left:0}.modal_editor{ left:10px;top:10px;right:10px;bottom:10px;position:fixed;z-index:100}.displayHTML a:visited{color:white}.loadingSpinner{width:32px;height:32px;display:block;padding:0px;background-image:url(/w/extensions/MwEmbedSupport/MwEmbedModules/MwEmbedSupport/skins/common/images/loading_ani.gif?a51c5)}.mw-imported-resource{border:thin solid black}.kaltura-icon{background-image:url(/w/extensions/MwEmbedSupport/MwEmbedModules/MwEmbedSupport/skins/common/images/kaltura_logo_sm_transparent.png?fa5c4) !important;background-repeat:no-repeat;display:block;height:12px;width:12px;margin-top:2px !important;margin-left:3px !important}.mw-fullscreen-overlay{background:rgb(0,0,0) none repeat scroll 0% 0%;position:fixed;top:0pt;left:0pt;width:100%;height:100%;-moz-background-clip:border;-moz-background-origin:padding;-moz-background-inline-policy:continuous} .play-btn-large{width:70px;height:53px;background :url(/w/extensions/MwEmbedSupport/MwEmbedModules/MwEmbedSupport/skins/common/images/player_big_play_button.png?69699);position :absolute;cursor :pointer;border :none !important; }.play-btn-large:hover{background :url(/w/extensions/MwEmbedSupport/MwEmbedModules/MwEmbedSupport/skins/common/images/player_big_play_button_hover.png?97c2a)}.carouselContainer{position :absolute;width :100%;z-index :2}.carouselVideoTitle{position :absolute;top :0px;left :0px;width :100%;background :rgba(0,0,0,0.8);color :white;font-size :small;font-weight :bold;z-index :2}.carouselVideoTitleText{display :block;padding :10px 10px 10px 20px}.carouselTitleDuration{position :absolute;top :0px;right :0px;padding :2px;background-color :#5A5A5A;color :#D9D9D9;font-size :smaller;z-index :2}.carouselImgTitle{position :absolute;width :100%;text-align :center;color :white;font-size :small;background :rgba(0,0,0,0.4)}.carouselImgDuration{position :absolute;top :2px;left :2px;background :rgba( 0,0,0,0.7 );color :white;padding :1px 6px;font-size :small}.carouselPrevButton,.carouselNextButton{display :block;position :absolute;bottom:23px}.carouselPrevButton{left :5px}.carouselNextButton{right:6px}.alert-container{border-radius:3px;background-image:-webkit-gradient(linear,left bottom,left top,color-stop(0.04,rgb(215,215,215)),color-stop(0.55,rgb(230,230,230)),color-stop(1,rgb(255,255,255)));background-image:-o-linear-gradient(bottom,rgb(215,215,215) 4%,rgb(230,230,230) 55%,rgb(255,255,255) 100%);background-image:-ms-linear-gradient(bottom,rgb(215,215,215) 4%,rgb(230,230,230) 55%,rgb(255,255,255) 100%);background-image:-moz-linear-gradient(bottom,rgb(215,215,215) 4%,rgb(230,230,230) 55%,rgb(255,255,255) 100%);background-image:-webkit-linear-gradient(bottom,rgb(215,215,215) 4%,rgb(230,230,230) 55%,rgb(255,255,255) 100%);background-image:linear-gradient(bottom,rgb(215,215,215) 4%,rgb(230,230,230) 55%,rgb(255,255,255) 100%);margin:auto;position:absolute;top:0;left:0;right:0;bottom:0;max-width:80%;max-height:30%}.alert-title{background-color :#E6E6E6;padding :5px;border-bottom :1px solid #D1D1D1;font-weight :normal !important;font-size:14px !important;border-top-left-radius:3px;border-top-right-radius:3px }.alert-message{padding :5px;font-weight :normal !important;text-align:center;font-size:14px !important}.alert-buttons-container{text-align:center;padding-bottom:5px}.alert-button{background-color:#474747;color:white;border-radius:.5em;padding:2px 10px;background-image:-webkit-gradient( linear,left bottom,left top,color-stop(0.04,rgb(25,25,25)),color-stop(0.55,rgb(47,47,47)),color-stop(0.68,rgb(71,71,71)) );background-image:-o-linear-gradient(bottom,rgb(25,25,25) 4%,rgb(47,47,47) 55%,rgb(71,71,71) 68%);background-image:-moz-linear-gradient(bottom,rgb(25,25,25) 4%,rgb(47,47,47) 55%,rgb(71,71,71) 68%);background-image:-ms-linear-gradient(bottom,rgb(25,25,25) 4%,rgb(47,47,47) 55%,rgb(71,71,71) 68%);background-image:-webkit-linear-gradient(bottom,rgb(25,25,25) 4%,rgb(47,47,47) 55%,rgb(71,71,71) 68%);background-image:linear-gradient(bottom,rgb(25,25,25) 4%,rgb(47,47,47) 55%,rgb(71,71,71) 68%)}.alert-text{color :black !important}
.PopUpMediaTransform a .play-btn-large{top:50%;left :50%;width:70px;height:53px;margin-left:-35px;margin-top:-25px;background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEYAAAA1CAYAAAD8mJ3rAAADNklEQVR4AWJAAhxA7AHEeUDcP8JwHtTvHAxoQAuI5wDxZkD75qBkSRKG0bZt21h7t23btm2bO7ZthNq2rq3mo0zWeKaty6yI8wB1Et+fEnDuIC4+SdEGvIBSPvP6oxOhYihjG8WImHtQxDaAk52GEeQFT4uBYqAYKAaKUVNTG9bX1x+FYj4iLi4+UFVVRXjz5s0Wwo0bN1geHh7zAi8mKSkJ9fLly63vOX/+POO3336bElgx////P/3Fixdbu9HS0kKxsLAYFzgxZ8+eZTx9+nRzP8rKygh6enqjAiOmt7eX/ujRo82DcP/+/fWsrCyskpLSEN+L6e7upoMf3jgMt27dWouKilqRkZEZ5FsxHR0dtDt37qwfhUuXLrG8vb0XxMTEBvhODJhcaSCi149DZ2cn3cHBYZavxDQ2NlKvXLmydhLU19dTfvjhhym+EFNXV0e5ePHi6klSVFREMDIyGuNpMdXV1WQQ2azTIDU1FaOpqTnCk2IqKirIPT09zNOiq6uLARIMpaCgMMhTYkpLS0kgmZinDZjk6SDBFiUlJQd4QkxhYSGxra2Nzi7AgpVibm4+zvVi8vLyiE1NTXR2gshBeg5Xi8nOziaAZKKxG7AwneBqMZmZmQSQTFR2o6OjM8LVYkCk4svLyynsJCYmBs31c0xycjKupKSEwg5A4Ud2c3NbkJCQGOB6MfHx8diCggLyaePr67ukqKg4xDN1DOjWWJBMpNMiODgYtb365QExoCrFgs0n4kkTHh6OBqcOYzy7VoqIiMCkp6cTT4ro6GgsKOAmeH51HRYWhgbJRDgusbGxODs7uylhYeF+vth2CAkJQYNkIhwVRMgvv/wyg5xP8dVGVVBQECoxMRF/WECa4f7666+57aU9n4gJCAhAIT95GBwcHObl5OQG+fqUwN/fH4UMh4Pg7u6+pKqqOsx+AVwqxsfHZ0VLS2tUoE4ikV6wm5DAwECUsbHxuECeXevq6o59LwREOMbKympyW/QKkhgEZJi4uLgsIr0H1CLT+0QvvFEFxUAxUAwUA8Xs+pYAviUohSK2UfrpvdJrKOMzbz86ef/9AIfU5yH0AyJklzeR8E3kO5B6nR5s2Y8qAAAAAElFTkSuQmCC);background-image:url(/w/extensions/TimedMediaHandler/resources/player_big_play_button.png?f49a4)!ie}.PopUpMediaTransform a .play-btn-large :hover{background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEYAAAA1CAYAAAD8mJ3rAAADSklEQVR4AeybA4wlWRRA1/u1tjG2g3YHY0/U4djTtm3btm3bTtu27du2WVXvJif8PIV3Ue+jBUECrgPsgAbBYAeuTzlYEicBU8CH4JhOuZiJ3wBHJGUOlyknEJxIxlLACYQ5ErEMcLLSZYRwRGKQGCQGicG0mP/++y/4/Pnz4UjMDCQSyQ+ianR0dGyS6urqbnZ29nTCi9HR0SkYHBwcW0peXl7Ho0ePEggrJisrq62/v390NWJjYxsZGRmjCCcmJyeno7u7e2Q9PDw8qs6ePRtOGDEZGRntEMMbobm5edDCwqLsjz/+CMK9mNTU1Db4w0Oboaampk9BQSH/22+/DcCtmMTExNb6+vrBrVBYWNjNzc2d+eWXX/rhTkx0dHRLZWXlwHZISkpqe/LkSSquxISHhzeXlJT07QShoaFNN2/ejMeFmODg4KaCgoLencTZ2bn68uXLEZgW4+/v3wBLdvduYGBgUHL06NFQTIrx9vauT09P79otUlJSOhQVFQt++eWXQEyJcXNzq4OVqXO3iYqKaoMVLJtGo/ljQoyjo2MtpP3tewUUrE0MDAzRB16MjY1NDaxMbXuJr69vE5VK9T/QYiDFrw4KCmrda6AwjTnQYkxMTKrg9G7ea06ePBl2oMXAklrp6enZtJcoKSkVY6FRVeHi4tK4F8CNvuHNmzdZFArF/8CLUVdXL7O3t6/fbXh4eHJ//fXXIMzkMcrKyqXW1tZ1u4WoqGjBkSNHQjGX+crLy5eamZnV7jTS0tLF586di8RsrSQjI1NiZGRUs1NA+l9KT08fg/nqWlJSslhfX796u6ioqJRfu3Yt/pNPPvHFRdtBTEysSFtbu2qrgJCKe/fuJZPJZH9cNapEREQKNTU1KzeLmppaBRsbW9qyohAvYgQFBQsm/+RmgDZmxo8//hiI6ykBPz9/weTlsBHevn2b+88//4TMvZ/oYri5ufOPHz8eTqhJ5Lt373JWEyIsLFx45cqVaELOrk+fPh0J2W/5QiGwhJewsrLGzS29RBQzyeRl8vLly+zJswdykUQSieSPHhxaHSQGiUFikBgkBu0lWH0vATcSsQzu2f1KLkjGHG4TA4cJFOjBs9RoFtIDBQiOPZGjeyIBuiuBC06Nbn0AAAAASUVORK5CYII=);background-image:url(/w/extensions/TimedMediaHandler/resources/player_big_play_button_hover.png?8a926)!ie} .mw-tmh-playtext{display:block;height:0;overflow:hidden}
@media print{#centralNotice{display:none}}.cn-closeButton{display:inline-block;zoom:1;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABMAAAATCAYAAAByUDbMAAACeklEQVR4Aa1UM3jkcRBN6qS//tTFttPHqmI359VZ5dm2L1zFtr02YlRx5sX2ft/7ed7O/w1M5ufnt8NZQjCBQXhG+IYZe5zjfju7xcHc3NzEzMwM6xOEqNnZ2ZeVlZW827dv1ycmJnbGx8d3Yr5582Z9eXk5D/d4h/empqYm+K0nw3yKcF4sFmdzOJzGgIAAhb29vc7Ozk6/Atrr/fz8lCwWq6m3tzcb72G3nmzFo/NNTU354eHhIltbW72Tk5M2IyND9P79+8ZPnz7VvXnzpoHBYPS4uLhobGxsDMHBwaLa2tp82MF+PVmUUqnMioiI6LOysjLQLCcPS2ZmZn7Ozc39oPtFYG80GgWpqakSS0tLY1hYmEgikWTBfoXsLD16BT3gUWhoqEKtVheREZ+Qu0IEjI+PZ2k0GqFer+cnJSWJra2tDWw2u2FqauoVeEAW3NzcnBcYGCh3dHTUtba2ltNjLvRJSEiQEQEPRENDQzkMBqPXwsKiXyqVFsFDNzc3LWmoqKmp4YIHZIyHDx9WOzg46GJiYmTk5e/h4eHstLQ0CX2ykXST4JPJ8y7sSVM5/QGMf9y4caMHf3r//v1a8IDsGRm04/D58+dtpNFPPNRqtflXrlzpI8G15IEGc2xsrFSlUglwD+Tk5NRBmuTk5A7wgOxbXFxcF8iysrIa1mskEolKvL29Ne7u7mpXV1dNaWlp9fr7X79+VYMMeQieY/dsv5p1r2g2Nja2vWZ7RXNiYuIA0dwlzwYGBjbkGXmURXeLeUa1ul2ebV8BEH+7CiAiASTYvgJ2qc3MzMzF2vz8+XPd27dvG5hMZg+CsV1tHmfXOP5+dqyddgHOI7v1srTdcwAAAABJRU5ErkJggg==);background:url(/w/extensions/CentralNotice/resources/subscribing/CloseWindow19x19.png?7596b)!ie;width:19px;height:19px;text-indent:19px;white-space:nowrap;overflow:hidden}
.cite-accessibility-label{position:absolute !important; top:-99999px;clip:rect(1px 1px 1px 1px); clip:rect(1px,1px,1px,1px);padding:0 !important;border:0 !important;height:1px !important;width:1px !important;overflow:hidden}
@media screen {
	.tochidden,.toctoggle{-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}.toctoggle{font-size:94%}}
@media print {
	#toc.tochidden,.toctoggle{display:none}}
cite,dfn{font-style:inherit} q{quotes:'"' '"' "'" "'"} blockquote{overflow:hidden;margin:1em 0;padding:0 40px} strong.selflink{font-weight:700} .mw-body sub,.mw-body sup,span.reference {font-size:80%} #interwiki-completelist{font-weight:bold}body.page-Main_Page #ca-delete{display:none !important}body.page-Main_Page #mp-topbanner{clear:both} .client-js .mw-special-Watchlist #watchlist-message{display:none} .updatedmarker{background-color:transparent;color:#006400} #toolbar{height:22px;margin-bottom:6px} #editpage-specialchars{display:none} body.action-info :target,.citation:target{background-color:#DEF; background-color:rgba(0,127,255,0.133)} .citation{word-wrap:break-word} @media screen,handheld{.citation .printonly{display:none}} ol.references,div.reflist,div.refbegin{font-size:90%; margin-bottom:0.5em}div.refbegin-100{font-size:100%; }div.reflist ol.references{font-size:100%; list-style-type:inherit; } sup.reference{font-weight:normal;font-style:normal} span.brokenref{display:none} div.columns{margin-top:0.3em}div.columns dl,div.columns ol,div.columns ul{margin-top:0} .nocolbreak,div.columns li,div.columns dd dd{-webkit-column-break-inside:avoid;page-break-inside:avoid;break-inside:avoid-column} .flowlist ul{overflow-x:hidden;margin-left:0;padding-left:1.6em}.flowlist ol{overflow-x:hidden;margin-left:0;padding-left:3.2em}.flowlist dl{overflow-x:hidden} .hlist dl,.hlist ol,.hlist ul{margin:0;padding:0} .hlist dd,.hlist dt,.hlist li{margin:0;display:inline} .hlist.inline,.hlist.inline dl,.hlist.inline ol,.hlist.inline ul,.hlist dl dl,.hlist dl ol,.hlist dl ul,.hlist ol dl,.hlist ol ol,.hlist ol ul,.hlist ul dl,.hlist ul ol,.hlist ul ul{display:inline} .hlist .mw-empty-li{display:none} .hlist dt:after{content:":"}.hlist dd:after,.hlist li:after{content:" · ";font-weight:bold}.hlist dd:last-child:after,.hlist dt:last-child:after,.hlist li:last-child:after{content:none} .hlist dd dd:first-child:before,.hlist dd dt:first-child:before,.hlist dd li:first-child:before,.hlist dt dd:first-child:before,.hlist dt dt:first-child:before,.hlist dt li:first-child:before,.hlist li dd:first-child:before,.hlist li dt:first-child:before,.hlist li li:first-child:before{content:" (";font-weight:normal}.hlist dd dd:last-child:after,.hlist dd dt:last-child:after,.hlist dd li:last-child:after,.hlist dt dd:last-child:after,.hlist dt dt:last-child:after,.hlist dt li:last-child:after,.hlist li dd:last-child:after,.hlist li dt:last-child:after,.hlist li li:last-child:after{content:") ";font-weight:normal} .hlist ol{counter-reset:listitem}.hlist ol > li{counter-increment:listitem}.hlist ol > li:before{content:" " counter(listitem) " ";white-space:nowrap}.hlist dd ol > li:first-child:before,.hlist dt ol > li:first-child:before,.hlist li ol > li:first-child:before{content:" (" counter(listitem) " "} .plainlist ol,.plainlist ul{line-height:inherit;list-style:none none;margin:0}.plainlist ol li,.plainlist ul li{margin-bottom:0} .navbox{ border:1px solid #aaa;width:100%;margin:auto;clear:both;font-size:88%;text-align:center;padding:1px}.navbox-inner,.navbox-subgroup{width:100%}.navbox-group,.navbox-title,.navbox-abovebelow{padding:0.25em 1em; line-height:1.5em;text-align:center}th.navbox-group{ white-space:nowrap; text-align:right}.navbox,.navbox-subgroup{background:#fdfdfd; }.navbox-list{line-height:1.5em;border-color:#fdfdfd; }.navbox th,.navbox-title{background:#ccccff; }.navbox-abovebelow,th.navbox-group,.navbox-subgroup .navbox-title{background:#ddddff; }.navbox-subgroup .navbox-group,.navbox-subgroup .navbox-abovebelow{background:#e6e6ff; }.navbox-even{background:#f7f7f7; }.navbox-odd{background:transparent; }table.navbox{margin-top:1em; }table.navbox table.navbox{margin-top:0; }table.navbox + table.navbox{margin-top:-1px; }.navbox .hlist td dl,.navbox .hlist td ol,.navbox .hlist td ul,.navbox td.hlist dl,.navbox td.hlist ol,.navbox td.hlist ul{padding:0.125em 0; } .navbar{display:inline;font-size:88%;font-weight:normal}.navbar ul{display:inline;white-space:nowrap}.mw-body-content .navbar ul{line-height:inherit}.navbar li{word-spacing:-0.125em}.navbar.mini li abbr[title]{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit} .infobox .navbar{font-size:100%}.navbox .navbar{display:block;font-size:100%}.navbox-title .navbar{ float:left; text-align:left; margin-right:0.5em;width:6em} .collapseButton{ float:right;font-weight:normal; margin-left:0.5em; text-align:right;width:auto} .navbox .collapseButton{width:6em} .mw-collapsible-toggle{font-weight:normal; text-align:right}.navbox .mw-collapsible-toggle{width:6em} .infobox{border:1px solid #aaa;border-spacing:3px;background-color:#f9f9f9;color:black; margin:0.5em 0 0.5em 1em;padding:0.2em; float:right; clear:right;font-size:88%;line-height:1.5em}.infobox caption{font-size:125%;font-weight:bold;padding:0.2em}.infobox td,.infobox th{vertical-align:top; text-align:left}.infobox.bordered{border-collapse:collapse}.infobox.bordered td,.infobox.bordered th{border:1px solid #aaa}.infobox.bordered .borderless td,.infobox.bordered .borderless th{border:0}.infobox.sisterproject{width:20em;font-size:90%}.infobox.standard-talk{border:1px solid #c0c090;background-color:#f8eaba}.infobox.standard-talk.bordered td,.infobox.standard-talk.bordered th{border:1px solid #c0c090} .infobox.bordered .mergedtoprow td,.infobox.bordered .mergedtoprow th{border:0;border-top:1px solid #aaa; border-right:1px solid #aaa}.infobox.bordered .mergedrow td,.infobox.bordered .mergedrow th{border:0; border-right:1px solid #aaa} .infobox.geography{border-collapse:collapse;line-height:1.2em;font-size:90%}.infobox.geography td,.infobox.geography th{border-top:1px solid #aaa;padding:0.4em 0.6em 0.4em 0.6em}.infobox.geography .mergedtoprow td,.infobox.geography .mergedtoprow th{border-top:1px solid #aaa;padding:0.4em 0.6em 0.2em 0.6em}.infobox.geography .mergedrow td,.infobox.geography .mergedrow th{border:0;padding:0 0.6em 0.2em 0.6em}.infobox.geography .mergedbottomrow td,.infobox.geography .mergedbottomrow th{border-top:0;border-bottom:1px solid #aaa;padding:0 0.6em 0.4em 0.6em}.infobox.geography .maptable td,.infobox.geography .maptable th{border:0;padding:0} .wikitable.plainrowheaders th[scope=row]{font-weight:normal; text-align:left} .wikitable td ul,.wikitable td ol,.wikitable td dl{ text-align:left} .toc.hlist ul,#toc.hlist ul,.wikitable.hlist td ul,.wikitable.hlist td ol,.wikitable.hlist td dl{text-align:inherit} div.listenlist{background:url(//upload.wikimedia.org/wikipedia/commons/4/47/Sound-icon.svg) no-repeat scroll 0 0 transparent;background-size:30px;padding-left:40px} table.mw-hiero-table td{vertical-align:middle} div.medialist{min-height:50px;margin:1em; background-position:top left;background-repeat:no-repeat}div.medialist ul{list-style-type:none;list-style-image:none;margin:0}div.medialist ul li{padding-bottom:0.5em}div.medialist ul li li{font-size:91%;padding-bottom:0} div#content a[href$=".pdf"].external,div#content a[href*=".pdf?"].external,div#content a[href*=".pdf#"].external,div#content a[href$=".PDF"].external,div#content a[href*=".PDF?"].external,div#content a[href*=".PDF#"].external,div#mw_content a[href$=".pdf"].external,div#mw_content a[href*=".pdf?"].external,div#mw_content a[href*=".pdf#"].external,div#mw_content a[href$=".PDF"].external,div#mw_content a[href*=".PDF?"].external,div#mw_content a[href*=".PDF#"].external{background:url(//upload.wikimedia.org/wikipedia/commons/2/23/Icons-mini-file_acrobat.gif) no-repeat right; padding-right:18px} div#content span.PDFlink a,div#mw_content span.PDFlink a{background:url(//upload.wikimedia.org/wikipedia/commons/2/23/Icons-mini-file_acrobat.gif) no-repeat right; padding-right:18px} div.columns-2 div.column{ float:left;width:50%;min-width:300px}div.columns-3 div.column{ float:left;width:33.3%;min-width:200px}div.columns-4 div.column{ float:left;width:25%;min-width:150px}div.columns-5 div.column{ float:left;width:20%;min-width:120px} .messagebox{border:1px solid #aaa;background-color:#f9f9f9;width:80%;margin:0 auto 1em auto;padding:.2em}.messagebox.merge{border:1px solid #c0b8cc;background-color:#f0e5ff;text-align:center}.messagebox.cleanup{border:1px solid #9f9fff;background-color:#efefff;text-align:center}.messagebox.standard-talk{border:1px solid #c0c090;background-color:#f8eaba;margin:4px auto} .mbox-inside .standard-talk,.messagebox.nested-talk{border:1px solid #c0c090;background-color:#f8eaba;width:100%;margin:2px 0;padding:2px}.messagebox.small{width:238px;font-size:85%; float:right;clear:both; margin:0 0 1em 1em;line-height:1.25em}.messagebox.small-talk{width:238px;font-size:85%; float:right;clear:both; margin:0 0 1em 1em;line-height:1.25em;background:#F8EABA} th.mbox-text,td.mbox-text{ border:none; padding:0.25em 0.9em; width:100%; }td.mbox-image{ border:none; padding:2px 0 2px 0.9em; text-align:center}td.mbox-imageright{ border:none; padding:2px 0.9em 2px 0; text-align:center}td.mbox-empty-cell{ border:none;padding:0;width:1px} table.ambox{margin:0 10%; border:1px solid #aaa; border-left:10px solid #1e90ff; background:#fbfbfb}table.ambox + table.ambox{ margin-top:-1px}.ambox th.mbox-text,.ambox td.mbox-text{ padding:0.25em 0.5em; }.ambox td.mbox-image{  padding:2px 0 2px 0.5em; }.ambox td.mbox-imageright{  padding:2px 0.5em 2px 0; }table.ambox-notice{ border-left:10px solid #1e90ff; }table.ambox-speedy{ border-left:10px solid #b22222; background:#fee; }table.ambox-delete{ border-left:10px solid #b22222; }table.ambox-content{ border-left:10px solid #f28500; }table.ambox-style{ border-left:10px solid #f4c430; }table.ambox-move{ border-left:10px solid #9932cc; }table.ambox-protection{ border-left:10px solid #bba; } table.imbox{margin:4px 10%;border-collapse:collapse;border:3px solid #1e90ff; background:#fbfbfb}.imbox .mbox-text .imbox{ margin:0 -0.5em; display:block; }.mbox-inside .imbox{ margin:4px}table.imbox-notice{border:3px solid #1e90ff; }table.imbox-speedy{border:3px solid #b22222; background:#fee; }table.imbox-delete{border:3px solid #b22222; }table.imbox-content{border:3px solid #f28500; }table.imbox-style{border:3px solid #f4c430; }table.imbox-move{border:3px solid #9932cc; }table.imbox-protection{border:3px solid #bba; }table.imbox-license{border:3px solid #88a; background:#f7f8ff; }table.imbox-featured{border:3px solid #cba135; } table.cmbox{margin:3px 10%;border-collapse:collapse;border:1px solid #aaa;background:#DFE8FF; }table.cmbox-notice{background:#D8E8FF; }table.cmbox-speedy{margin-top:4px;margin-bottom:4px;border:4px solid #b22222; background:#FFDBDB; }table.cmbox-delete{background:#FFDBDB; }table.cmbox-content{background:#FFE7CE; }table.cmbox-style{background:#FFF9DB; }table.cmbox-move{background:#E4D8FF; }table.cmbox-protection{background:#EFEFE1; } table.ombox{margin:4px 10%;border-collapse:collapse;border:1px solid #aaa; background:#f9f9f9}table.ombox-notice{border:1px solid #aaa; }table.ombox-speedy{border:2px solid #b22222; background:#fee; }table.ombox-delete{border:2px solid #b22222; }table.ombox-content{border:1px solid #f28500; }table.ombox-style{border:1px solid #f4c430; }table.ombox-move{border:1px solid #9932cc; }table.ombox-protection{border:2px solid #bba; } table.tmbox{margin:4px 10%;border-collapse:collapse;border:1px solid #c0c090; background:#f8eaba}.mediawiki .mbox-inside .tmbox{ margin:2px 0; width:100%; }.mbox-inside .tmbox.mbox-small{ line-height:1.5em; font-size:100%; }table.tmbox-speedy{border:2px solid #b22222; background:#fee; }table.tmbox-delete{border:2px solid #b22222; }table.tmbox-content{border:2px solid #f28500; }table.tmbox-style{border:2px solid #f4c430; }table.tmbox-move{border:2px solid #9932cc; }table.tmbox-protection,table.tmbox-notice{border:1px solid #c0c090; } table.dmbox{clear:both;margin:0.9em 1em;border-top:1px solid #ccc;border-bottom:1px solid #ccc;background:transparent} table.fmbox{clear:both;margin:0.2em 0;width:100%;border:1px solid #aaa;background:#f9f9f9; }table.fmbox-system{background:#f9f9f9}table.fmbox-warning{border:1px solid #bb7070; background:#ffdbdb; }table.fmbox-editnotice{background:transparent} div.mw-warning-with-logexcerpt,div.mw-lag-warn-high,div.mw-cascadeprotectedwarning,div#mw-protect-cascadeon,div.titleblacklist-warning,div.locked-warning{clear:both;margin:0.2em 0;border:1px solid #bb7070;background:#ffdbdb;padding:0.25em 0.9em} div.mw-lag-warn-normal,div.fmbox-system{clear:both;margin:0.2em 0;border:1px solid #aaa;background:#f9f9f9;padding:0.25em 0.9em} body.mediawiki table.mbox-small{  clear:right; float:right; margin:4px 0 4px 1em;width:238px;font-size:88%;line-height:1.25em}body.mediawiki table.mbox-small-left{  margin:4px 1em 4px 0;width:238px;border-collapse:collapse;font-size:88%;line-height:1.25em}  .compact-ambox table .mbox-image,.compact-ambox table .mbox-imageright,.compact-ambox table .mbox-empty-cell{display:none} .compact-ambox table.ambox{border:none;border-collapse:collapse;background:transparent;margin:0 0 0 1.6em !important;padding:0 !important;width:auto;display:block}body.mediawiki .compact-ambox table.mbox-small-left{font-size:100%;width:auto;margin:0} .compact-ambox table .mbox-text{padding:0 !important;margin:0 !important}.compact-ambox table .mbox-text-span{display:list-item;line-height:1.5em;list-style-type:square;list-style-image:url(//en.wikipedia.org/w/skins/MonoBook/bullet.gif)}.skin-vector .compact-ambox table .mbox-text-span{list-style-type:disc;list-style-image:url(//en.wikipedia.org/w/skins/Vector/images/bullet-icon.svg);list-style-image:url(//en.wikipedia.org/w/skins/Vector/images/bullet-icon.png)\9} .compact-ambox .hide-when-compact{display:none} div.noarticletext{border:none;background:transparent;padding:0} .visualhide{position:absolute;left:-10000px;top:auto;width:1px;height:1px;overflow:hidden} #wpSave{font-weight:bold} .hiddenStructure{display:inline !important;color:#f00;background-color:#0f0} .check-icon a.new{display:none;speak:none} .nounderlines a,.IPA a:link,.IPA a:visited{text-decoration:none !important} div.NavFrame{margin:0;padding:4px;border:1px solid #aaa;text-align:center;border-collapse:collapse;font-size:95%}div.NavFrame + div.NavFrame{border-top-style:none;border-top-style:hidden}div.NavPic{background-color:#fff;margin:0;padding:2px; float:left}div.NavFrame div.NavHead{line-height:1.6em;font-weight:bold;background-color:#ccf;position:relative}div.NavFrame p,div.NavFrame div.NavContent,div.NavFrame div.NavContent p{font-size:100%}div.NavEnd{margin:0;padding:0;line-height:1px;clear:both}a.NavToggle{position:absolute;top:0; right:3px;font-weight:normal;font-size:90%} .hatnote{font-style:italic}.hatnote i{font-style:normal}div.hatnote{ padding-left:1.6em;margin-bottom:0.5em}div.hatnote + div.hatnote{margin-top:-0.5em} .listify td{display:list-item}.listify tr{display:block}.listify table{display:block} .geo-default,.geo-dms,.geo-dec{display:inline}.geo-nondefault,.geo-multi-punct{display:none}.longitude,.latitude{white-space:nowrap} .hlist .tocnumber,.hlist .toctext{display:inline} .nonumtoc .tocnumber{display:none}.nonumtoc #toc ul,.nonumtoc .toc ul{line-height:1.5em;list-style:none none;margin:.3em 0 0;padding:0}.hlist.nonumtoc #toc ul ul,.hlist.nonumtoc .toc ul ul{ margin:0} .toclimit-2 .toclevel-1 ul,.toclimit-3 .toclevel-2 ul,.toclimit-4 .toclevel-3 ul,.toclimit-5 .toclevel-4 ul,.toclimit-6 .toclevel-5 ul,.toclimit-7 .toclevel-6 ul{display:none} blockquote.templatequote div.templatequotecite{line-height:1.5em; text-align:left; padding-left:1.6em;margin-top:0} div.user-block{padding:5px;margin-bottom:0.5em;border:1px solid #A9A9A9;background-color:#FFEFD5} .nowrap,.nowraplinks a,.nowraplinks .selflink,sup.reference a{white-space:nowrap}.nowrap pre{white-space:pre} .wrap,.wraplinks a{white-space:normal} .template-documentation{clear:both;margin:1em 0 0 0;border:1px solid #aaa;background-color:#ecfcf4;padding:1em} .imagemap-inline div{display:inline} #wpUploadDescription{height:13em} .thumbinner{min-width:100px} div.thumb .thumbimage{background-color:#fff} div#content .gallerybox div.thumb{ background-color:#F9F9F9} .gallerybox .thumb img{background:#fff url(//upload.wikimedia.org/wikipedia/commons/5/5d/Checker-16x16.png) repeat} .ns-0 .gallerybox .thumb img,.ns-2 .gallerybox .thumb img,.ns-100 .gallerybox .thumb img,.nochecker .gallerybox .thumb img{background:#fff} #mw-subcategories,#mw-pages,#mw-category-media,#filehistory,#wikiPreview,#wikiDiff{clear:both}body.rtl #mw-articlefeedbackv5,body.rtl #mw-articlefeedback{display:block; margin-bottom:1em; clear:right;  float:right; } .wpb .wpb-header{display:none}.wpbs-inner .wpb .wpb-header{display:block} .wpbs-inner .wpb .wpb-header{display:table-row} .wpbs-inner .wpb-outside{display:none}  .mw-tag-markers{font-style:italic;font-size:90%} .sysop-show,.accountcreator-show,.templateeditor-show,.autoconfirmed-show{display:none} .ve-ui-mwNoticesPopupTool-item .editnotice-redlink,.mw-ve-editNotice .editnotice-redlink{display:none !important} ul.permissions-errors > li{list-style:none none}ul.permissions-errors{margin:0} body.page-Special_UserLogin .mw-label label,body.page-Special_UserLogin_signup .mw-label label{white-space:nowrap} .transborder{border:solid transparent} .times-serif,span.texhtml{font-family:"Nimbus Roman No9 L","Times New Roman",Times,serif;font-size:118%;line-height:1}span.texhtml{white-space:nowrap}span.texhtml span.texhtml{font-size:100%}span.mwe-math-mathml-inline{font-size:118%} .digits,.texhtml{-moz-font-feature-settings:"lnum","tnum","kern" 0;-webkit-font-feature-settings:"lnum","tnum","kern" 0;font-feature-settings:"lnum","tnum","kern" 0;font-variant-numeric:lining-nums tabular-nums;font-kerning:none} .mwe-math-fallback-image-display,.mwe-math-mathml-display{margin-left:1.6em !important;margin-top:0.6em;margin-bottom:0.6em}.mwe-math-mathml-display math{display:inline} table#mw-prefixindex-list-table,table#mw-prefixindex-nav-table{width:98%} .portal-column-left{float:left;width:50%}.portal-column-right{float:right;width:49%}.portal-column-left-wide{float:left;width:60%}.portal-column-right-narrow{float:right;width:39%}.portal-column-left-extra-wide{float:left;width:70%}.portal-column-right-extra-narrow{float:right;width:29%}@media only screen and (max-width:800px){ .portal-column-left,.portal-column-right,.portal-column-left-wide,.portal-column-right-narrow,.portal-column-left-extra-wide,.portal-column-right-extra-narrow{float:inherit;width:inherit}} #bodyContent .letterhead{background-image:url(//upload.wikimedia.org/wikipedia/commons/e/e0/Tan-page-corner.png);background-repeat:no-repeat;padding:2em;background-color:#faf9f2} .treeview ul{padding:0;margin:0}.treeview li{padding:0;margin:0;list-style-type:none;list-style-image:none}.treeview li li{background:url(//upload.wikimedia.org/wikipedia/commons/f/f2/Treeview-grey-line.png) no-repeat 0 -2981px; padding-left:20px;text-indent:0.3em}.treeview li li.lastline{background-position:0 -5971px }.treeview li.emptyline > ul{ margin-left:-1px}.treeview li.emptyline > ul > li:first-child{background-position:0 9px } td .sortkey,th .sortkey{display:none;speak:none} .inputbox-hidecheckboxes form .inputbox-element{display:none !important} .k-player .k-attribution{visibility:hidden} .PopUpMediaTransform a .play-btn-large{margin:0;top:auto;right:auto;bottom:0;left:0} .mw-ve-editNotice .mbox-image{display:none}
.page-Main_Page #deleteconfirm,.page-Main_Page #t-cite,.page-Main_Page #footer-info-lastmod,.action-view.page-Main_Page #siteSub,.action-view.page-Main_Page #contentSub,.action-view.page-Main_Page .firstHeading{display:none !important} #coordinates{position:absolute;top:0;right:0;float:right;margin:0;padding:0;line-height:1.5em;text-align:right;text-indent:0;font-size:85%;text-transform:none;white-space:nowrap} div.flaggedrevs_short{position:absolute;top:-3em;right:100px;z-index:1} div.vectorMenu div{z-index:2} #siteSub{display:block;font-size:92%} .mw-body .mw-indicators{padding-top:0.4em}
@media print {
	.ns-0 .ambox,.ns-0 .navbox,.ns-0 .vertical-navbox,.ns-0 .infobox.sisterproject,.ns-0 .hatnote,.ns-0 .dablink,.ns-0 .metadata,.editlink,.navbar,a.NavToggle,span.collapseButton,span.mw-collapsible-toggle,th .sortkey,td .sortkey{display:none !important} #content cite a.external.text:after,.nourlexpansion a.external.text:after,.nourlexpansion a.external.autonumber:after{display:none !important} table.collapsible tr,div.NavPic,div.NavContent{display:block !important}table.collapsible tr{display:table-row !important} #firstHeading{margin:0} #content a.external.text:after,#content a.external.autonumber:after{word-wrap:break-word}}
.mw-collapsible-toggle{float:right;-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}  .mw-content-ltr .mw-collapsible-toggle,.mw-content-rtl .mw-content-ltr .mw-collapsible-toggle{float:right} .mw-content-rtl .mw-collapsible-toggle,.mw-content-ltr .mw-content-rtl .mw-collapsible-toggle{float:left}.mw-customtoggle,.mw-collapsible-toggle{cursor:pointer} caption .mw-collapsible-toggle,.mw-content-ltr caption .mw-collapsible-toggle,.mw-content-rtl caption .mw-collapsible-toggle,.mw-content-rtl .mw-content-ltr caption .mw-collapsible-toggle,.mw-content-ltr .mw-content-rtl caption .mw-collapsible-toggle{float:none} li .mw-collapsible-toggle,.mw-content-ltr li .mw-collapsible-toggle,.mw-content-rtl li .mw-collapsible-toggle,.mw-content-rtl .mw-content-ltr li .mw-collapsible-toggle,.mw-content-ltr .mw-content-rtl li .mw-collapsible-toggle{float:none} .mw-collapsible-toggle-li{list-style:none}
.suggestions{overflow:hidden;position:absolute;top:0;left:0;width:0;border:none;z-index:1099;padding:0;margin:-1px 0 0 0}.suggestions-special{position:relative;background-color:white;cursor:pointer;border:solid 1px #aaaaaa;padding:0;margin:0;margin-top:-2px;display:none;padding:0.25em 0.25em;line-height:1.25em}.suggestions-results{background-color:white;cursor:pointer;border:solid 1px #aaaaaa;padding:0;margin:0}.suggestions-result{color:black;margin:0;line-height:1.5em;padding:0.01em 0.25em;text-align:left; overflow:hidden;-o-text-overflow:ellipsis; text-overflow:ellipsis;white-space:nowrap}.suggestions-result-current{background-color:#4C59A6;color:white}.suggestions-special .special-label{color:gray;text-align:left}.suggestions-special .special-query{color:black;font-style:italic;text-align:left}.suggestions-special .special-hover{background-color:silver}.suggestions-result-current .special-label,.suggestions-result-current .special-query{color:white}.highlight{font-weight:bold}
.wp-teahouse-question-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}#wp-th-question-ask{float:right}.wp-teahouse-ask a.external{background-image:none !important}.wp-teahouse-respond-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}.wp-th-respond{float:right}.wp-teahouse-respond a.external{background-image:none !important}
.referencetooltip{position:absolute;list-style:none;list-style-image:none;opacity:0;font-size:10px;margin:0;z-index:5;padding:0}.referencetooltip li{border:#080086 2px solid;max-width:260px;padding:10px 8px 13px 8px;margin:0px;background-color:#F7F7F7;-webkit-box-shadow:2px 4px 2px rgba(0,0,0,0.3);-moz-box-shadow:2px 4px 2px rgba(0,0,0,0.3);box-shadow:2px 4px 2px rgba(0,0,0,0.3)}.referencetooltip li+li{margin-left:7px;margin-top:-2px;border:0;padding:0;height:3px;width:0px;background-color:transparent;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none;border-top:12px #080086 solid;border-right:7px transparent solid;border-left:7px transparent solid}.referencetooltip>li+li::after{content:'';border-top:8px #F7F7F7 solid;border-right:5px transparent solid;border-left:5px transparent solid;margin-top:-12px;margin-left:-5px;z-index:1;height:0px;width:0px;display:block}.client-js body .referencetooltip li li{border:none;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none;height:auto;width:auto;margin:auto;padding:0;position:static}.RTflipped{padding-top:13px}.referencetooltip.RTflipped li+li{position:absolute;top:2px;border-top:0;border-bottom:12px #080086 solid}.referencetooltip.RTflipped li+li::after{border-top:0;border-bottom:8px #F7F7F7 solid;position:absolute;margin-top:7px}.RTsettings{float:right;height:24px;width:24px;cursor:pointer;background-image:url(//upload.wikimedia.org/wikipedia/commons/thumb/7/77/Gear_icon.svg/24px-Gear_icon.svg.png);background-image:linear-gradient(transparent,transparent),url(//upload.wikimedia.org/wikipedia/commons/7/77/Gear_icon.svg);margin-top:-9px;margin-right:-7px;-webkit-transition:opacity 0.15s;-moz-transition:opacity 0.15s;-ms-transition:opacity 0.15s;-o-transition:opacity 0.15s;transition:opacity 0.15s;opacity:0.6;filter:alpha(opacity=60)}.RTsettings:hover{opacity:1;filter:alpha(opacity=100)}.RTTarget{border:#080086 2px solid}
.skin-vector li.GA,.skin-monobook li.GA,.skin-modern li.GA{list-style-image:url(//upload.wikimedia.org/wikipedia/commons/4/42/Monobook-bullet-ga.png)} .skin-vector li.FA,.skin-monobook li.FA{list-style-image:url(//upload.wikimedia.org/wikipedia/commons/d/d4/Monobook-bullet-star.png)}.skin-modern li.FA{list-style-image:url(//upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Modern-bullet-star.svg/9px-Modern-bullet-star.svg.png)}
.mw-ui-button{font-family:inherit;font-size:1em;display:inline-block;min-width:4em;max-width:28.75em;padding:.5em 1em;margin:0;border-radius:2px;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;-webkit-appearance:none;*display:inline;zoom:1;vertical-align:middle;background:#ffffff;color:#555555;border:1px solid #cccccc;text-align:center;font-weight:bold;cursor:pointer}.mw-ui-button:hover{background-color:#cccccc}.mw-ui-button:focus{border-color:#ffffff;box-shadow:0 0 0 1px #cccccc;outline:none}.mw-ui-button:focus::-moz-focus-inner{border-color:transparent}.mw-ui-button:active,.mw-ui-button.is-on,.mw-ui-button.mw-ui-checked{background:#777777;box-shadow:none}.mw-ui-button:hover,.mw-ui-button:active,.mw-ui-button:visited{color:#555555}.mw-ui-button:focus{background-color:#cccccc}.mw-ui-button:disabled{color:#cccccc}.mw-ui-button:disabled:hover,.mw-ui-button:disabled:active{background:#ffffff;box-shadow:none}.mw-ui-button:disabled{text-shadow:none;cursor:default}.mw-ui-button.mw-ui-big{font-size:1.3em}.mw-ui-button.mw-ui-block{display:block;width:100%;margin-left:auto;margin-right:auto}.mw-ui-button.mw-ui-progressive,.mw-ui-button.mw-ui-primary{background:#347bff;color:#fff;border:1px solid #347bff;text-shadow:0 1px rgba(0,0,0,0.1)}.mw-ui-button.mw-ui-progressive:hover,.mw-ui-button.mw-ui-primary:hover{background-color:#2962cc}.mw-ui-button.mw-ui-progressive:focus,.mw-ui-button.mw-ui-primary:focus{border-color:#ffffff;box-shadow:0 0 0 1px #2962cc;outline:none}.mw-ui-button.mw-ui-progressive:focus::-moz-focus-inner,.mw-ui-button.mw-ui-primary:focus::-moz-focus-inner{border-color:transparent}.mw-ui-button.mw-ui-progressive:active,.mw-ui-button.mw-ui-primary:active,.mw-ui-button.mw-ui-progressive.is-on,.mw-ui-button.mw-ui-primary.is-on,.mw-ui-button.mw-ui-progressive.mw-ui-checked,.mw-ui-button.mw-ui-primary.mw-ui-checked{background:#2962cc;box-shadow:none}.mw-ui-button.mw-ui-progressive:disabled,.mw-ui-button.mw-ui-primary:disabled{background:#dddddd;border-color:#dddddd}.mw-ui-button.mw-ui-progressive:disabled:hover,.mw-ui-button.mw-ui-primary:disabled:hover,.mw-ui-button.mw-ui-progressive:disabled:active,.mw-ui-button.mw-ui-primary:disabled:active,.mw-ui-button.mw-ui-progressive:disabled.mw-ui-checked,.mw-ui-button.mw-ui-primary:disabled.mw-ui-checked{box-shadow:none}.mw-ui-button.mw-ui-progressive.mw-ui-quiet,.mw-ui-button.mw-ui-primary.mw-ui-quiet{color:#555555}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:hover,.mw-ui-button.mw-ui-primary.mw-ui-quiet:hover,.mw-ui-button.mw-ui-progressive.mw-ui-quiet:focus,.mw-ui-button.mw-ui-primary.mw-ui-quiet:focus{background:transparent;color:#347bff}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:active,.mw-ui-button.mw-ui-primary.mw-ui-quiet:active,.mw-ui-button.mw-ui-progressive.mw-ui-quiet.mw-ui-checked,.mw-ui-button.mw-ui-primary.mw-ui-quiet.mw-ui-checked{color:#2962cc}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:disabled,.mw-ui-button.mw-ui-primary.mw-ui-quiet:disabled{color:#cccccc}.mw-ui-button.mw-ui-constructive{background:#347bff;color:#fff;border:1px solid #347bff;text-shadow:0 1px rgba(0,0,0,0.1)}.mw-ui-button.mw-ui-constructive:hover{background-color:#2962cc}.mw-ui-button.mw-ui-constructive:focus{border-color:#ffffff;box-shadow:0 0 0 1px #2962cc;outline:none}.mw-ui-button.mw-ui-constructive:focus::-moz-focus-inner{border-color:transparent}.mw-ui-button.mw-ui-constructive:active,.mw-ui-button.mw-ui-constructive.is-on,.mw-ui-button.mw-ui-constructive.mw-ui-checked{background:#2962cc;box-shadow:none}.mw-ui-button.mw-ui-constructive:disabled{background:#dddddd;border-color:#dddddd}.mw-ui-button.mw-ui-constructive:disabled:hover,.mw-ui-button.mw-ui-constructive:disabled:active,.mw-ui-button.mw-ui-constructive:disabled.mw-ui-checked{box-shadow:none}.mw-ui-button.mw-ui-constructive.mw-ui-quiet{color:#555555}.mw-ui-button.mw-ui-constructive.mw-ui-quiet:hover,.mw-ui-button.mw-ui-constructive.mw-ui-quiet:focus{background:transparent;color:#347bff}.mw-ui-button.mw-ui-constructive.mw-ui-quiet:active,.mw-ui-button.mw-ui-constructive.mw-ui-quiet.mw-ui-checked{color:#2962cc}.mw-ui-button.mw-ui-constructive.mw-ui-quiet:disabled{color:#cccccc}.mw-ui-button.mw-ui-destructive{background:#d11d13;color:#fff;border:1px solid #d11d13;text-shadow:0 1px rgba(0,0,0,0.1)}.mw-ui-button.mw-ui-destructive:hover{background-color:#a7170f}.mw-ui-button.mw-ui-destructive:focus{border-color:#ffffff;box-shadow:0 0 0 1px #a7170f;outline:none}.mw-ui-button.mw-ui-destructive:focus::-moz-focus-inner{border-color:transparent}.mw-ui-button.mw-ui-destructive:active,.mw-ui-button.mw-ui-destructive.is-on,.mw-ui-button.mw-ui-destructive.mw-ui-checked{background:#a7170f;box-shadow:none}.mw-ui-button.mw-ui-destructive:disabled{background:#dddddd;border-color:#dddddd}.mw-ui-button.mw-ui-destructive:disabled:hover,.mw-ui-button.mw-ui-destructive:disabled:active,.mw-ui-button.mw-ui-destructive:disabled.mw-ui-checked{box-shadow:none}.mw-ui-button.mw-ui-destructive.mw-ui-quiet{color:#555555}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:hover,.mw-ui-button.mw-ui-destructive.mw-ui-quiet:focus{background:transparent;color:#d11d13}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:active,.mw-ui-button.mw-ui-destructive.mw-ui-quiet.mw-ui-checked{color:#a7170f}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:disabled{color:#cccccc}.mw-ui-button.mw-ui-quiet{background:transparent;border:0;text-shadow:none;color:#555555}.mw-ui-button.mw-ui-quiet:hover,.mw-ui-button.mw-ui-quiet:focus{background:transparent;color:#555555}.mw-ui-button.mw-ui-quiet:active,.mw-ui-button.mw-ui-quiet.mw-ui-checked{color:#777777}.mw-ui-button.mw-ui-quiet:disabled{color:#cccccc}.mw-ui-button.mw-ui-quiet:hover,.mw-ui-button.mw-ui-quiet:focus{box-shadow:none}.mw-ui-button.mw-ui-quiet:active,.mw-ui-button.mw-ui-quiet:disabled{background:transparent}a.mw-ui-button{text-decoration:none;line-height:normal}a.mw-ui-button:hover,a.mw-ui-button:focus{text-decoration:none}.mw-ui-button-group > *{min-width:48px;border-radius:0;float:left}.mw-ui-button-group > *:first-child{border-top-left-radius:2px;border-bottom-left-radius:2px}.mw-ui-button-group > *:not( :first-child ){border-left:0}.mw-ui-button-group > *:last-child{border-top-right-radius:2px;border-bottom-right-radius:2px}.mw-ui-button-group .is-on .button{cursor:default}
.mw-ui-icon{position:relative;min-height:1.5em;min-width:1.5em}.mw-ui-icon.mw-ui-icon-element{text-indent:-999px;overflow:hidden;width:3.5em;min-width:3.5em;max-width:3.5em}.mw-ui-icon.mw-ui-icon-element:before{left:0;right:0;position:absolute;margin:0 1em}.mw-ui-icon.mw-ui-icon-before:before,.mw-ui-icon.mw-ui-icon-element:before{background-position:50% 50%;background-repeat:no-repeat;background-size:100% auto;float:left;display:block;min-height:1.5em;content:''}.mw-ui-icon.mw-ui-icon-before:before{position:relative;width:1.5em;margin-right:1em}
#p-lang .uls-settings-trigger{background:transparent no-repeat right top;background-image:url(/w/extensions/UniversalLanguageSelector/resources/images/cog-sprite.png?30312);background-image:linear-gradient(transparent,transparent),url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%3F%3E%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20xmlns%3Axlink%3D%22http%3A%2F%2Fwww.w3.org%2F1999%2Fxlink%22%20width%3D%2214%22%20height%3D%2232%22%3E%3Cdefs%3E%3Cpath%20d%3D%22M14%209.3V6.73l-1.575-.264c-.117-.44-.292-.848-.496-1.2l.93-1.285-1.81-1.84-1.31.908c-.38-.205-.79-.38-1.196-.497L8.284%201H5.716l-.263%201.578c-.437.117-.816.293-1.196.497L2.975%202.17%201.137%203.98l.934%201.287c-.2.38-.376.79-.493%201.228L0%206.73V9.3l1.575.264c.117.438.292.818.496%201.198l-.93%201.315L2.95%2013.89l1.312-.938c.38.205.787.38%201.224.497L5.746%2015h2.566l.263-1.578c.408-.117.817-.293%201.196-.497l1.315.935%201.81-1.812-.935-1.315c.203-.38.38-.76.495-1.2L14%209.303zm-7%201.404c-1.488%200-2.683-1.2-2.683-2.69S5.542%205.327%207%205.327c1.458%200%202.683%201.198%202.683%202.69%200%201.49-1.195%202.688-2.683%202.688z%22%20id%3D%22a%22%2F%3E%3C%2Fdefs%3E%3Cuse%20xlink%3Ahref%3D%22%23a%22%20fill%3D%22%23808080%22%2F%3E%3Cuse%20transform%3D%22translate%280%2016%29%22%20xlink%3Ahref%3D%22%23a%22%20fill%3D%22%23555%22%2F%3E%3C%2Fsvg%3E%0A);background-image:linear-gradient(transparent,transparent),url(/w/extensions/UniversalLanguageSelector/resources/images/cog-sprite.svg?624e4)!ie;background-image:-o-linear-gradient(transparent,transparent),url(/w/extensions/UniversalLanguageSelector/resources/images/cog-sprite.png?30312);height:16px;width:14px;float:right;cursor:pointer}.skin-vector #p-lang .uls-settings-trigger{ margin-top:3px}#p-lang .uls-settings-trigger:hover{background-position:right -16px}</style><style>
.ve-activated #toc,.ve-activated #siteNotice,.ve-activated .mw-indicators, .ve-active #bodyContent > :not( #siteSub ):not( #contentSub ):not( .ve-ui-mwTocWidget ),.ve-activated #t-print,.ve-activated #t-permalink,.ve-activated #p-coll-print_export,.ve-activated #t-cite,.ve-deactivating .ve-ui-surface{display:none} .ve-activating .ve-ui-surface{height:0;overflow:hidden}.ve-activated #bodyContent,.ve-activated #firstHeading{opacity:0.6;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none} .ve-activated #content{position:relative}.ve-init-mw-desktopArticleTarget-loading-overlay{position:absolute;left:0;right:0;z-index:1;margin-top:-0.5em}.ve-init-mw-desktopArticleTarget-progress{height:1em;overflow:hidden;margin:0 25%}.ve-init-mw-desktopArticleTarget-progress-bar{height:1em;width:0} .mw-editsection{white-space:nowrap; unicode-bidi:-moz-isolate;unicode-bidi:-webkit-isolate;unicode-bidi:isolate}.mw-editsection-divider{color:#555} .ve-init-mw-desktopArticleTarget-progress{height:0.75em;border:1px solid #347bff;background:#fff;border-radius:2px;box-shadow:0 0.1em 0 0 rgba(0,0,0,0.15) }.ve-init-mw-desktopArticleTarget-progress-bar{height:0.75em;background:#347bff}
.suggestions a.mw-searchSuggest-link,.suggestions a.mw-searchSuggest-link:hover,.suggestions a.mw-searchSuggest-link:active,.suggestions a.mw-searchSuggest-link:focus{color:black;text-decoration:none}.suggestions-result-current a.mw-searchSuggest-link,.suggestions-result-current a.mw-searchSuggest-link:hover,.suggestions-result-current a.mw-searchSuggest-link:active,.suggestions-result-current a.mw-searchSuggest-link:focus{color:white}.suggestions a.mw-searchSuggest-link .special-query{ overflow:hidden;-o-text-overflow:ellipsis; text-overflow:ellipsis;white-space:nowrap}
.mw-mmv-overlay{position:fixed;top:0px;left:0px;right:0px;bottom:0px;z-index:1000;background-color:#000000}body.mw-mmv-lightbox-open{overflow-y:auto}body.mw-mmv-lightbox-open #mw-page-base,body.mw-mmv-lightbox-open #mw-head-base,body.mw-mmv-lightbox-open #mw-navigation,body.mw-mmv-lightbox-open #content,body.mw-mmv-lightbox-open #footer,body.mw-mmv-lightbox-open #globalWrapper{ display:none}body.mw-mmv-lightbox-open > *{ display:none}body.mw-mmv-lightbox-open > .mw-mmv-overlay,body.mw-mmv-lightbox-open > .mw-mmv-wrapper{display:block}.mw-mmv-filepage-buttons{margin-top:5px}.mw-mmv-filepage-buttons .mw-mmv-view-expanded,.mw-mmv-filepage-buttons .mw-mmv-view-config{display:block;line-height:inherit}.mw-mmv-filepage-buttons .mw-mmv-view-expanded.mw-ui-icon:before{background-image:url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%201024%20768%22%3E%0A%20%20%20%20%3Cg%20fill%3D%22%23777%22%3E%0A%20%20%20%20%20%20%20%20%3Cpath%20d%3D%22M851.2%2071.6L690.7%20232.1l-40.1-40.3-9.6%20164.8%20164.8-9.3-40.3-40.4L926%20146.4l58.5%2058.5L997.6%200%20792.7%2013.1%22%2F%3E%0A%20%20%20%20%20%20%20%20%3Cpath%20d%3D%22M769.6%2089.3H611.9l70.9%2070.8%207.9%207.5m-47.1%20234.6l-51.2%203%203-51.2%209.4-164.4%205.8-100.3H26.4V768h883.1V387l-100.9%205.8-165%209.4zM813.9%20678H113.6l207.2-270.2%2031.5-12.9L548%20599.8l105.9-63.2%20159.8%20140.8.2.6zm95.6-291.9V228l-79.1%2078.9%207.8%207.9%22%2F%3E%0A%20%20%20%20%3C%2Fg%3E%0A%3C%2Fsvg%3E%0A);background-image:url(/w/extensions/MultimediaViewer/resources/mmv/img/expand.svg?b714e)!ie}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before{background-image:url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22UTF-8%22%20standalone%3D%22no%22%3F%3E%0A%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%201024%20768%22%3E%0A%20%20%20%20%3Cpath%20d%3D%22M897%20454.6V313.4L810.4%20299c-6.4-23.3-16-45.7-27.3-65.8l50.5-71.4-99.4-100.2-71.4%2050.5c-20.9-11.2-42.5-20.9-65.8-27.3L582.6-1H441.4L427%2085.6c-23.3%206.4-45.7%2016-65.8%2027.3l-71.4-50.5-100.3%2099.5%2050.5%2071.4c-11.2%2020.9-20.9%2042.5-27.3%2066.6L127%20313.4v141.2l85.8%2014.4c6.4%2023.3%2016%2045.7%2027.3%2066.6L189.6%20607l99.5%2099.5%2071.4-50.5c20.9%2011.2%2042.5%2020.9%2066.6%2027.3l14.4%2085.8h141.2l14.4-86.6c23.3-6.4%2045.7-16%2065.8-27.3l71.4%2050.5%2099.5-99.5-50.5-71.4c11.2-20.9%2020.9-42.5%2027.3-66.6l86.4-13.6zm-385%2077c-81.8%200-147.6-66.6-147.6-147.6%200-81.8%2066.6-147.6%20147.6-147.6S659.6%20302.2%20659.6%20384%20593.8%20531.6%20512%20531.6z%22%20fill%3D%22%23777%22%2F%3E%0A%3C%2Fsvg%3E%0A);background-image:url(/w/extensions/MultimediaViewer/resources/mmv/img/gear_gray.svg?330ae)!ie;opacity:0.75}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before:hover{opacity:1}</style><meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="./Hidden Markov model - Wikipedia, the free encyclopedia_files/load(1).php">
<script async="" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/load(2).php"></script>
<meta name="generator" content="MediaWiki 1.28.0-wmf.1">
<meta name="referrer" content="origin-when-cross-origin">
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Hidden_Markov_model">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit">
<link rel="edit" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit">
<link rel="apple-touch-icon" href="https://en.wikipedia.org/static/apple-touch/wikipedia.png">
<link rel="shortcut icon" href="https://en.wikipedia.org/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="https://en.wikipedia.org/w/opensearch_desc.php" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://en.wikipedia.org/w/api.php?action=rsd">
<link rel="copyright" href="https://creativecommons.org/licenses/by-sa/3.0/">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Hidden_Markov_model">
<link rel="dns-prefetch" href="https://meta.wikimedia.org/">
<style type="text/css">:root #content > #center > .dose > .dosesingle,
:root #content > #right > .dose > .dosesingle
{display:none !important;}</style><script src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/load(3).php"></script></head>
<body class="mediawiki ltr sitedir-ltr ns-0 ns-subject page-Hidden_Markov_model rootpage-Hidden_Markov_model skin-vector action-view" cz-shortcut-listen="true"><div id="StayFocusd-infobar" style="display: none; top: 0px;">
    <img src="chrome-extension://laankejkbhbdhmipfmgcngdelahlfoji/common/img/eye_19x19_red.png">
    <span id="StayFocusd-infobar-msg"></span>
    <span id="StayFocusd-infobar-links">
        <a id="StayFocusd-infobar-never-show">hide forever</a>&nbsp;&nbsp;|&nbsp;&nbsp;
        <a id="StayFocusd-infobar-hide">hide once</a>
    </span>
</div>
		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>

							<div id="siteNotice"><div id="centralNotice"></div><!-- CentralNotice --></div>
						<div class="mw-indicators">
</div>
			<h1 id="firstHeading" class="firstHeading" lang="en">Hidden Markov model</h1>
									<div id="bodyContent" class="mw-body-content">
									<div id="siteSub">From Wikipedia, the free encyclopedia</div>
								<div id="contentSub"></div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#mw-head">navigation</a>, 					<a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%">
<tbody><tr>
<th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br>
<a href="https://en.wikipedia.org/wiki/Data_mining" title="Data mining">data mining</a></th>
</tr>
<tr>
<td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="https://en.wikipedia.org/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/220px-Kernel_Machine.svg.png" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="232"></a></td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame1">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems<a class="NavToggle" id="NavToggle1" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#">[show]</a></div>
<div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="https://en.wikipedia.org/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="https://en.wikipedia.org/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame2">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
<div style="padding:0.1em 0;line-height:1.2em;"><a href="https://en.wikipedia.org/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br>
<span style="font-weight:normal;"><small style="font-size:85%;">(<b><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&nbsp;• <b><a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</small></span></div>
<a class="NavToggle" id="NavToggle2" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#">[show]</a></div>
<div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a> (<a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a>, <a href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a>, <a href="https://en.wikipedia.org/wiki/Random_forest" title="Random forest">Random forest</a>)</li>
<li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">Neural networks</a></li>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="https://en.wikipedia.org/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame3">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a><a class="NavToggle" id="NavToggle3" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#">[show]</a></div>
<div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="https://en.wikipedia.org/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" title="Expectation-maximization algorithm" class="mw-redirect">Expectation-maximization (EM)</a></li>
<li><br>
<a href="https://en.wikipedia.org/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="https://en.wikipedia.org/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mean-shift" title="Mean-shift" class="mw-redirect">Mean-shift</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame4">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a><a class="NavToggle" id="NavToggle4" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#">[show]</a></div>
<div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Canonical_correlation_analysis" title="Canonical correlation analysis" class="mw-redirect">CCA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame5">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a><a class="NavToggle" id="NavToggle5" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#">[show]</a></div>
<div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Graphical_model" title="Graphical model">Graphical models</a> (<a href="https://en.wikipedia.org/wiki/Bayesian_network" title="Bayesian network">Bayes net</a>, <a href="https://en.wikipedia.org/wiki/Conditional_random_field" title="Conditional random field">CRF</a>, <strong class="selflink">HMM</strong>)</li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame6">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a><a class="NavToggle" id="NavToggle6" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#">[show]</a></div>
<div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_classification" title="K-nearest neighbors classification" class="mw-redirect"><i>k</i>-NN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame7">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">Neural nets</a><a class="NavToggle" id="NavToggle7" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#">[show]</a></div>
<div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame8">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory<a class="NavToggle" id="NavToggle8" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#">[show]</a></div>
<div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Bias-variance_dilemma" title="Bias-variance dilemma" class="mw-redirect">Bias-variance dilemma</a></li>
<li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame9">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine learning venues<a class="NavToggle" id="NavToggle9" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#">[show]</a></div>
<div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a></li>
<li><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a rel="nofollow" class="external text" href="http://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td class="plainlist" style="padding:0.3em 0.4em 0.3em;font-weight:bold;border-top: 1px solid #aaa; border-bottom: 1px solid #aaa;border-top:1px solid #aaa;border-bottom:1px solid #aaa;">
<ul>
<li><span class="metadata"><a href="https://en.wikipedia.org/wiki/File:Portal-puzzle.svg" class="image"><img alt="Portal icon" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/16px-Portal-puzzle.svg.png" width="16" height="14" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28"></a></span> <a href="https://en.wikipedia.org/wiki/Portal:Machine_learning" title="Portal:Machine learning">Machine learning portal</a></li>
</ul>
</td>
</tr>
<tr>
<td style="text-align:right;font-size:115%;padding-top: 0.6em;">
<div class="plainlinks hlist navbar mini">
<ul>
<li class="nv-view"><a href="https://en.wikipedia.org/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li>
<li class="nv-talk"><a href="https://en.wikipedia.org/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li>
<li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li>
</ul>
</div>
</td>
</tr>
</tbody></table>
<p>A <b>hidden Markov model</b> (<b>HMM</b>) is a <a href="https://en.wikipedia.org/wiki/Statistical_model" title="Statistical model">statistical</a> <a href="https://en.wikipedia.org/wiki/Markov_model" title="Markov model">Markov model</a> in which the system being modeled is assumed to be a <a href="https://en.wikipedia.org/wiki/Markov_process" title="Markov process">Markov process</a> with unobserved (<i>hidden</i>) states. An HMM can be presented as the simplest <a href="https://en.wikipedia.org/wiki/Dynamic_Bayesian_network" title="Dynamic Bayesian network">dynamic Bayesian network</a>. The mathematics behind the HMM were developed by <a href="https://en.wikipedia.org/wiki/Leonard_E._Baum" title="Leonard E. Baum">L. E. Baum</a> and coworkers.<sup id="cite_ref-1" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-1">[1]</a></sup><sup id="cite_ref-2" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-2">[2]</a></sup><sup id="cite_ref-3" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-3">[3]</a></sup><sup id="cite_ref-4" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-4">[4]</a></sup><sup id="cite_ref-5" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-5">[5]</a></sup> It is closely related to an earlier work on the optimal nonlinear <a href="https://en.wikipedia.org/wiki/Filtering_problem_(stochastic_processes)" title="Filtering problem (stochastic processes)">filtering problem</a> by <a href="https://en.wikipedia.org/wiki/Ruslan_L._Stratonovich" title="Ruslan L. Stratonovich" class="mw-redirect">Ruslan L. Stratonovich</a>,<sup id="cite_ref-Stratonovich1960_6-0" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-Stratonovich1960-6">[6]</a></sup> who was the first to describe the <a href="https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm" title="Forward–backward algorithm">forward-backward procedure</a>.</p>
<p>In simpler <a href="https://en.wikipedia.org/wiki/Markov_model" title="Markov model">Markov models</a> (like a <a href="https://en.wikipedia.org/wiki/Markov_chain" title="Markov chain">Markov chain</a>), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters. In a <i>hidden</i> Markov model, the state is not directly visible, but the output, dependent on the state, is visible. Each state has a probability distribution over the possible output tokens. Therefore, the sequence of tokens generated by an HMM gives some information about the sequence of states. The adjective 'hidden' refers to the state sequence through which the model passes, not to the parameters of the model; the model is still referred to as a 'hidden' Markov model even if these parameters are known exactly.</p>
<p>Hidden Markov models are especially known for their application in <a href="https://en.wikipedia.org/wiki/Time" title="Time">temporal</a> pattern recognition such as <a href="https://en.wikipedia.org/wiki/Speech_recognition" title="Speech recognition">speech</a>, <a href="https://en.wikipedia.org/wiki/Handwriting_recognition" title="Handwriting recognition">handwriting</a>, <a href="https://en.wikipedia.org/wiki/Gesture_recognition" title="Gesture recognition">gesture recognition</a>,<sup id="cite_ref-7" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-7">[7]</a></sup> <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" title="Part-of-speech tagging">part-of-speech tagging</a>, musical score following,<sup id="cite_ref-8" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-8">[8]</a></sup> <a href="https://en.wikipedia.org/wiki/Partial_discharge" title="Partial discharge">partial discharges</a><sup id="cite_ref-9" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-9">[9]</a></sup> and <a href="https://en.wikipedia.org/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>.</p>
<p>A hidden Markov model can be considered a generalization of a <a href="https://en.wikipedia.org/wiki/Mixture_model" title="Mixture model">mixture model</a> where the hidden variables (or <a href="https://en.wikipedia.org/wiki/Latent_variables" title="Latent variables" class="mw-redirect">latent variables</a>), which control the mixture component to be selected for each observation, are related through a Markov process rather than independent of each other. Recently, hidden Markov models have been generalized to pairwise Markov models and triplet Markov models which allow consideration of more complex data structures <sup id="cite_ref-TMMEV_10-0" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-TMMEV-10">[10]</a></sup><sup id="cite_ref-JASP_11-0" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-JASP-11">[11]</a></sup> and the modelling of nonstationary data.<sup id="cite_ref-TSP_12-0" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-TSP-12">[12]</a></sup><sup id="cite_ref-SPL_13-0" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-SPL-13">[13]</a></sup></p>
<p></p>
<div id="toc" class="toc">
<div id="toctitle">
<h2>Contents</h2>
<span class="toctoggle">&nbsp;[<a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#" id="togglelink">hide</a>]&nbsp;</span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Description_in_terms_of_urns"><span class="tocnumber">1</span> <span class="toctext">Description in terms of urns</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Architecture"><span class="tocnumber">2</span> <span class="toctext">Architecture</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Inference"><span class="tocnumber">3</span> <span class="toctext">Inference</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Probability_of_an_observed_sequence"><span class="tocnumber">3.1</span> <span class="toctext">Probability of an observed sequence</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Probability_of_the_latent_variables"><span class="tocnumber">3.2</span> <span class="toctext">Probability of the latent variables</span></a>
<ul>
<li class="toclevel-3 tocsection-6"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Filtering"><span class="tocnumber">3.2.1</span> <span class="toctext">Filtering</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Smoothing"><span class="tocnumber">3.2.2</span> <span class="toctext">Smoothing</span></a></li>
<li class="toclevel-3 tocsection-8"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Most_likely_explanation"><span class="tocnumber">3.2.3</span> <span class="toctext">Most likely explanation</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-9"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Statistical_significance"><span class="tocnumber">3.3</span> <span class="toctext">Statistical significance</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#A_concrete_example"><span class="tocnumber">4</span> <span class="toctext">A concrete example</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Learning"><span class="tocnumber">5</span> <span class="toctext">Learning</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Mathematical_description"><span class="tocnumber">6</span> <span class="toctext">Mathematical description</span></a>
<ul>
<li class="toclevel-2 tocsection-13"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#General_description"><span class="tocnumber">6.1</span> <span class="toctext">General description</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Compared_with_a_simple_mixture_model"><span class="tocnumber">6.2</span> <span class="toctext">Compared with a simple mixture model</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Examples"><span class="tocnumber">6.3</span> <span class="toctext">Examples</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#A_two-level_Bayesian_HMM"><span class="tocnumber">6.4</span> <span class="toctext">A two-level Bayesian HMM</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-17"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Applications"><span class="tocnumber">7</span> <span class="toctext">Applications</span></a></li>
<li class="toclevel-1 tocsection-18"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#History"><span class="tocnumber">8</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-19"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Types"><span class="tocnumber">9</span> <span class="toctext">Types</span></a></li>
<li class="toclevel-1 tocsection-20"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Extensions"><span class="tocnumber">10</span> <span class="toctext">Extensions</span></a></li>
<li class="toclevel-1 tocsection-21"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#See_also"><span class="tocnumber">11</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-22"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#References"><span class="tocnumber">12</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-23"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#External_links"><span class="tocnumber">13</span> <span class="toctext">External links</span></a>
<ul>
<li class="toclevel-2 tocsection-24"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Concepts"><span class="tocnumber">13.1</span> <span class="toctext">Concepts</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#Software"><span class="tocnumber">13.2</span> <span class="toctext">Software</span></a></li>
</ul>
</li>
</ul>
</div>
<p></p>
<h2><span class="mw-headline" id="Description_in_terms_of_urns">Description in terms of urns</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=1" title="Edit section: Description in terms of urns">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright">
<div class="thumbinner" style="width:302px;"><a href="https://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg" class="image"><img alt="" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/300px-HiddenMarkovModel.svg.png" width="300" height="240" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/8a/HiddenMarkovModel.svg/450px-HiddenMarkovModel.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/8a/HiddenMarkovModel.svg/600px-HiddenMarkovModel.svg.png 2x" data-file-width="750" data-file-height="600"></a>
<div class="thumbcaption">
<div class="magnify"><a href="https://en.wikipedia.org/wiki/File:HiddenMarkovModel.svg" class="internal" title="Enlarge"></a></div>
Figure 1. Probabilistic parameters of a hidden Markov model (example)<br>
<i>X</i> — states<br>
<i>y</i> — possible observations<br>
<i>a</i> — state transition probabilities<br>
<i>b</i> — output probabilities</div>
</div>
</div>
<p>In its discrete form, a hidden Markov process can be visualized as a generalization of the <a href="https://en.wikipedia.org/wiki/Urn_problem" title="Urn problem">Urn problem</a> with replacement (where each item from the urn is returned to the original urn before the next step).<sup id="cite_ref-14" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-14">[14]</a></sup> Consider this example: in a room that is not visible to an observer there is a genie. The room contains urns X1, X2, X3, … each of which contains a known mix of balls, each ball labeled y1, y2, y3, … . The genie chooses an urn in that room and randomly draws a ball from that urn. It then puts the ball onto a conveyor belt, where the observer can observe the sequence of the balls but not the sequence of urns from which they were drawn. The genie has some procedure to choose urns; the choice of the urn for the <i>n</i>-th ball depends only upon a random number and the choice of the urn for the (<i>n</i>&nbsp;−&nbsp;1)-th ball. The choice of urn does not directly depend on the urns chosen before this single previous urn; therefore, this is called a <a href="https://en.wikipedia.org/wiki/Markov_process" title="Markov process">Markov process</a>. It can be described by the upper part of Figure 1.</p>
<p>The Markov process itself cannot be observed, only the sequence of labeled balls, thus this arrangement is called a "hidden Markov process". This is illustrated by the lower part of the diagram shown in Figure 1, where one can see that balls y1, y2, y3, y4 can be drawn at each state. Even if the observer knows the composition of the urns and has just observed a sequence of three balls, <i>e.g.</i> y1, y2 and y3 on the conveyor belt, the observer still cannot be <i>sure</i> which urn (<i>i.e.</i>, at which state) the genie has drawn the third ball from. However, the observer can work out other information, such as the likelihood that the third ball came from each of the urns.</p>
<h2><span class="mw-headline" id="Architecture">Architecture</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=2" title="Edit section: Architecture">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The diagram below shows the general architecture of an instantiated HMM. Each oval shape represents a random variable that can adopt any of a number of values. The random variable <i>x</i>(<i>t</i>) is the hidden state at time <span class="texhtml mvar" style="font-style:italic;">t</span> (with the model from the above diagram, <i>x</i>(<i>t</i>)&nbsp;∈&nbsp;{&nbsp;<i>x</i><sub>1</sub>,&nbsp;<i>x</i><sub>2</sub>,&nbsp;<i>x</i><sub>3</sub>&nbsp;}). The random variable <i>y</i>(<i>t</i>) is the observation at time <span class="texhtml mvar" style="font-style:italic;">t</span> (with <i>y</i>(<i>t</i>)&nbsp;∈&nbsp;{&nbsp;<i>y</i><sub>1</sub>,&nbsp;<i>y</i><sub>2</sub>,&nbsp;<i>y</i><sub>3</sub>,&nbsp;<i>y</i><sub>4</sub>&nbsp;}). The arrows in the diagram (often called a <a href="https://en.wikipedia.org/wiki/Trellis_(graph)" title="Trellis (graph)">trellis diagram</a>) denote conditional dependencies.</p>
<p>From the diagram, it is clear that the <a href="https://en.wikipedia.org/wiki/Conditional_probability_distribution" title="Conditional probability distribution">conditional probability distribution</a> of the hidden variable <i>x</i>(<i>t</i>) at time <span class="texhtml mvar" style="font-style:italic;">t</span>, given the values of the hidden variable <span class="texhtml mvar" style="font-style:italic;">x</span> at all times, depends <i>only</i> on the value of the hidden variable <i>x</i>(<i>t</i>&nbsp;−&nbsp;1); the values at time <i>t</i>&nbsp;−&nbsp;2 and before have no influence. This is called the <a href="https://en.wikipedia.org/wiki/Markov_property" title="Markov property">Markov property</a>. Similarly, the value of the observed variable <i>y</i>(<i>t</i>) only depends on the value of the hidden variable <i>x</i>(<i>t</i>) (both at time <span class="texhtml mvar" style="font-style:italic;">t</span>).</p>
<p>In the standard type of hidden Markov model considered here, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a <a href="https://en.wikipedia.org/wiki/Categorical_distribution" title="Categorical distribution">categorical distribution</a>) or continuous (typically from a <a href="https://en.wikipedia.org/wiki/Gaussian_distribution" title="Gaussian distribution" class="mw-redirect">Gaussian distribution</a>). The parameters of a hidden Markov model are of two types, <i>transition probabilities</i> and <i>emission probabilities</i> (also known as <i>output probabilities</i>). The transition probabilities control the way the hidden state at time <span class="texhtml mvar" style="font-style:italic;">t</span> is chosen given the hidden state at time <img class="mwe-math-fallback-image-inline tex" alt="t-1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f3e90ce87a25538f5a4be79a0a7c0fa5.png">.</p>
<p>The hidden state space is assumed to consist of one of <span class="texhtml mvar" style="font-style:italic;">N</span> possible values, modeled as a categorical distribution. (See the section below on extensions for other possibilities.) This means that for each of the <span class="texhtml mvar" style="font-style:italic;">N</span> possible states that a hidden variable at time <span class="texhtml mvar" style="font-style:italic;">t</span> can be in, there is a transition probability from this state to each of the <span class="texhtml mvar" style="font-style:italic;">N</span> possible states of the hidden variable at time <img class="mwe-math-fallback-image-inline tex" alt="t+1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43c98a64bcde4857b095743482e04281.png">, for a total of <img class="mwe-math-fallback-image-inline tex" alt="N^2" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/6828feb224035a25980fcbdb76126b02.png"> transition probabilities. Note that the set of transition probabilities for transitions from any given state must sum to 1. Thus, the <img class="mwe-math-fallback-image-inline tex" alt="N \times N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/395a9af17f8642e02a32af8637542947.png"> matrix of transition probabilities is a <a href="https://en.wikipedia.org/wiki/Stochastic_matrix" title="Stochastic matrix">Markov matrix</a>. Because any one transition probability can be determined once the others are known, there are a total of <img class="mwe-math-fallback-image-inline tex" alt="N(N-1)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/59ea204c42ec679928131491dab6b447.png"> transition parameters.</p>
<p>In addition, for each of the <span class="texhtml mvar" style="font-style:italic;">N</span> possible states, there is a set of emission probabilities governing the distribution of the observed variable at a particular time given the state of the hidden variable at that time. The size of this set depends on the nature of the observed variable. For example, if the observed variable is discrete with <span class="texhtml mvar" style="font-style:italic;">M</span> possible values, governed by a <a href="https://en.wikipedia.org/wiki/Categorical_distribution" title="Categorical distribution">categorical distribution</a>, there will be <img class="mwe-math-fallback-image-inline tex" alt="M-1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/7f8e22c19b2513613769168973edab29.png"> separate parameters, for a total of <img class="mwe-math-fallback-image-inline tex" alt="N(M-1)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/9a82835ccd17c1da0398346c7ce10dee.png"> emission parameters over all hidden states. On the other hand, if the observed variable is an <span class="texhtml mvar" style="font-style:italic;">M</span>-dimensional vector distributed according to an arbitrary <a href="https://en.wikipedia.org/wiki/Multivariate_Gaussian_distribution" title="Multivariate Gaussian distribution" class="mw-redirect">multivariate Gaussian distribution</a>, there will be <span class="texhtml mvar" style="font-style:italic;">M</span> parameters controlling the <a href="https://en.wikipedia.org/wiki/Mean" title="Mean">means</a> and <img class="mwe-math-fallback-image-inline tex" alt="\frac {M(M+1)} 2" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/29636e51cb7244269e43916d6f9e3209.png"> parameters controlling the <a href="https://en.wikipedia.org/wiki/Covariance_matrix" title="Covariance matrix">covariance matrix</a>, for a total of <img class="mwe-math-fallback-image-inline tex" alt="N \left(M + \frac{M(M+1)}{2}\right) = \frac {NM(M+3)} 2 = O(NM^2)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/eba254b2b4e9625a80913fe0823c2fdd.png"> emission parameters. (In such a case, unless the value of <span class="texhtml mvar" style="font-style:italic;">M</span> is small, it may be more practical to restrict the nature of the covariances between individual elements of the observation vector, e.g. by assuming that the elements are independent of each other, or less restrictively, are independent of all but a fixed number of adjacent elements.)</p>
<div class="center">
<div class="floatnone"><a href="https://en.wikipedia.org/wiki/File:Hmm_temporal_bayesian_net.svg" class="image" title="Temporal evolution of a hidden Markov model"><img alt="Temporal evolution of a hidden Markov model" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/500px-Hmm_temporal_bayesian_net.svg.png" width="500" height="128" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/83/Hmm_temporal_bayesian_net.svg/750px-Hmm_temporal_bayesian_net.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/83/Hmm_temporal_bayesian_net.svg/1000px-Hmm_temporal_bayesian_net.svg.png 2x" data-file-width="833" data-file-height="214"></a></div>
</div>
<h2><span class="mw-headline" id="Inference">Inference</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=3" title="Edit section: Inference">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright">
<div class="thumbinner" style="width:402px;"><a href="https://en.wikipedia.org/wiki/File:HMMsequence.svg" class="image"><img alt="" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/400px-HMMsequence.svg.png" width="400" height="414" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/13/HMMsequence.svg/600px-HMMsequence.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/13/HMMsequence.svg/800px-HMMsequence.svg.png 2x" data-file-width="727" data-file-height="753"></a>
<div class="thumbcaption">
<div class="magnify"><a href="https://en.wikipedia.org/wiki/File:HMMsequence.svg" class="internal" title="Enlarge"></a></div>
The state transition and output probabilities of an HMM are indicated by the line opacity in the upper part of the diagram. Given that we have observed the output sequence in the lower part of the diagram, we may be interested in the most likely sequence of states that could have produced it. Based on the arrows that are present in the diagram, the following state sequences are candidates:<br>
5 3 2 5 3 2<br>
4 3 2 5 3 2<br>
3 1 2 5 3 2<br>
We can find the most likely sequence by evaluating the joint probability of both the state sequence and the observations for each case (simply by multiplying the probability values, which here correspond to the opacities of the arrows involved). In general, this type of problem (i.e. finding the most likely explanation for an observation sequence) can be solved efficiently using the <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" title="Viterbi algorithm">Viterbi algorithm</a>.</div>
</div>
</div>
<p>Several <a href="https://en.wikipedia.org/wiki/Inference" title="Inference">inference</a> problems are associated with hidden Markov models, as outlined below.</p>
<h3><span class="mw-headline" id="Probability_of_an_observed_sequence">Probability of an observed sequence</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=4" title="Edit section: Probability of an observed sequence">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The task is to compute in a best way, given the parameters of the model, the probability of a particular output sequence. This requires summation over all possible state sequences:</p>
<p>The probability of observing a sequence</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="Y=y(0), y(1),\dots,y(L-1)\," src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/32161b2a33b88e9203eef348f96d0606.png"></dd>
</dl>
<p>of length <i>L</i> is given by</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="P(Y)=\sum_{X}P(Y\mid X)P(X),\," src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/722fb3185ebffcdc5c6097ec1c33427d.png"></dd>
</dl>
<p>where the sum runs over all possible hidden-node sequences</p>
<dl>
<dd><img class="mwe-math-fallback-image-inline tex" alt="X=x(0), x(1), \dots, x(L-1).\," src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/9fdd9f243b3a6b9238df822870180127.png"></dd>
</dl>
<p>Applying the principle of <a href="https://en.wikipedia.org/wiki/Dynamic_programming" title="Dynamic programming">dynamic programming</a>, this problem, too, can be handled efficiently using the <a href="https://en.wikipedia.org/wiki/Forward_algorithm" title="Forward algorithm">forward algorithm</a>.</p>
<h3><span class="mw-headline" id="Probability_of_the_latent_variables">Probability of the latent variables</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=5" title="Edit section: Probability of the latent variables">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A number of related tasks ask about the probability of one or more of the latent variables, given the model's parameters and a sequence of observations <img class="mwe-math-fallback-image-inline tex" alt="y(1),\dots,y(t)." src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c95e56967adc6917f613e7dc4176774f.png"></p>
<h4><span class="mw-headline" id="Filtering">Filtering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=6" title="Edit section: Filtering">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The task is to compute, given the model's parameters and a sequence of observations, the distribution over hidden states of the last latent variable at the end of the sequence, i.e. to compute <img class="mwe-math-fallback-image-inline tex" alt="P(x(t)\ |\ y(1),\dots,y(t))" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/318b0738e2f0c766421e9850a954e616.png">. This task is normally used when the sequence of latent variables is thought of as the underlying states that a process moves through at a sequence of points of time, with corresponding observations at each point in time. Then, it is natural to ask about the state of the process at the end.</p>
<p>This problem can be handled efficiently using the <a href="https://en.wikipedia.org/wiki/Forward_algorithm" title="Forward algorithm">forward algorithm</a>.</p>
<h4><span class="mw-headline" id="Smoothing">Smoothing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=7" title="Edit section: Smoothing">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>This is similar to filtering but asks about the distribution of a latent variable somewhere in the middle of a sequence, i.e. to compute <img class="mwe-math-fallback-image-inline tex" alt="P(x(k)\ |\ y(1), \dots, y(t))" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/1d46f7ac3da74ccc135fed941c626c1e.png"> for some <img class="mwe-math-fallback-image-inline tex" alt="k &lt; t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/74e03e13df14d80dcf5096cd9c746c04.png">. From the perspective described above, this can be thought of as the probability distribution over hidden states for a point in time <i>k</i> in the past, relative to time <i>t</i>.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Forward-backward_algorithm" title="Forward-backward algorithm" class="mw-redirect">forward-backward algorithm</a> is an efficient method for computing the smoothed values for all hidden state variables.</p>
<h4><span class="mw-headline" id="Most_likely_explanation">Most likely explanation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=8" title="Edit section: Most likely explanation">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The task, unlike the previous two, asks about the <a href="https://en.wikipedia.org/wiki/Joint_probability" title="Joint probability" class="mw-redirect">joint probability</a> of the <i>entire</i> sequence of hidden states that generated a particular sequence of observations (see illustration on the right). This task is generally applicable when HMM's are applied to different sorts of problems from those for which the tasks of filtering and smoothing are applicable. An example is <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" title="Part-of-speech tagging">part-of-speech tagging</a>, where the hidden states represent the underlying <a href="https://en.wikipedia.org/wiki/Part_of_speech" title="Part of speech">parts of speech</a> corresponding to an observed sequence of words. In this case, what is of interest is the entire sequence of parts of speech, rather than simply the part of speech for a single word, as filtering or smoothing would compute.</p>
<p>This task requires finding a maximum over all possible state sequences, and can be solved efficiently by the <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" title="Viterbi algorithm">Viterbi algorithm</a>.</p>
<h3><span class="mw-headline" id="Statistical_significance">Statistical significance</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=9" title="Edit section: Statistical significance">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>For some of the above problems, it may also be interesting to ask about <a href="https://en.wikipedia.org/wiki/Statistical_significance" title="Statistical significance">statistical significance</a>. What is the probability that a sequence drawn from some <a href="https://en.wikipedia.org/wiki/Null_distribution" title="Null distribution">null distribution</a> will have an HMM probability (in the case of the forward algorithm) or a maximum state sequence probability (in the case of the Viterbi algorithm) at least as large as that of a particular output sequence?<sup id="cite_ref-15" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-15">[15]</a></sup> When an HMM is used to evaluate the relevance of a hypothesis for a particular output sequence, the statistical significance indicates the <a href="https://en.wikipedia.org/wiki/False_positive_rate" title="False positive rate">false positive rate</a> associated with failing to reject the hypothesis for the output sequence.</p>
<h2><span class="mw-headline" id="A_concrete_example">A concrete example</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=10" title="Edit section: A concrete example">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Consider two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather where Bob lives, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.</p>
<p>Alice believes that the weather operates as a discrete <a href="https://en.wikipedia.org/wiki/Markov_chain" title="Markov chain">Markov chain</a>. There are two states, "Rainy" and "Sunny", but she cannot observe them directly, that is, they are <i>hidden</i> from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: "walk", "shop", or "clean". Since Bob tells Alice about his activities, those are the <i>observations</i>. The entire system is that of a hidden Markov model (HMM).</p>
<p>Alice knows the general weather trends in the area, and what Bob likes to do on average. In other words, the parameters of the HMM are known. They can be represented as follows in <a href="https://en.wikipedia.org/wiki/Python_programming_language" title="Python programming language" class="mw-redirect">Python</a>:</p>
<div class="mw-highlight mw-content-ltr" dir="ltr">
<pre><span class="n">states</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'Rainy'</span><span class="p">,</span> <span class="s1">'Sunny'</span><span class="p">)</span>
 
<span class="n">observations</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'walk'</span><span class="p">,</span> <span class="s1">'shop'</span><span class="p">,</span> <span class="s1">'clean'</span><span class="p">)</span>
 
<span class="n">start_probability</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Rainy'</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">'Sunny'</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">}</span>
 
<span class="n">transition_probability</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s1">'Rainy'</span> <span class="p">:</span> <span class="p">{</span><span class="s1">'Rainy'</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span> <span class="s1">'Sunny'</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">},</span>
   <span class="s1">'Sunny'</span> <span class="p">:</span> <span class="p">{</span><span class="s1">'Rainy'</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span> <span class="s1">'Sunny'</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">},</span>
   <span class="p">}</span>
 
<span class="n">emission_probability</span> <span class="o">=</span> <span class="p">{</span>
   <span class="s1">'Rainy'</span> <span class="p">:</span> <span class="p">{</span><span class="s1">'walk'</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">'shop'</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span> <span class="s1">'clean'</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">},</span>
   <span class="s1">'Sunny'</span> <span class="p">:</span> <span class="p">{</span><span class="s1">'walk'</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span> <span class="s1">'shop'</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s1">'clean'</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">},</span>
   <span class="p">}</span>
</pre></div>
<p>In this piece of code, <code>start_probability</code> represents Alice's belief about which state the HMM is in when Bob first calls her (all she knows is that it tends to be rainy on average). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) approximately <code>{'Rainy': 0.57, 'Sunny': 0.43}</code>. The <code>transition_probability</code> represents the change of the weather in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow will be sunny if today is rainy. The <code>emission_probability</code> represents how likely Bob is to perform a certain activity on each day. If it is rainy, there is a 50% chance that he is cleaning his apartment; if it is sunny, there is a 60% chance that he is outside for a walk.</p>
<div class="center">
<div class="floatnone"><a href="https://en.wikipedia.org/wiki/File:HMMGraph.svg" class="image" title="Graphical representation of the given HMM"><img alt="Graphical representation of the given HMM" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/400px-HMMGraph.svg.png" width="400" height="308" class="thumbborder" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/43/HMMGraph.svg/600px-HMMGraph.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/43/HMMGraph.svg/800px-HMMGraph.svg.png 2x" data-file-width="708" data-file-height="546"></a></div>
</div>
<p><i>A similar example is further elaborated in the <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm#Example" title="Viterbi algorithm">Viterbi algorithm</a> page.</i></p>
<h2><span class="mw-headline" id="Learning">Learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=11" title="Edit section: Learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The parameter learning task in HMMs is to find, given an output sequence or a set of such sequences, the best set of state transition and emission probabilities. The task is usually to derive the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood" title="Maximum likelihood">maximum likelihood</a> estimate of the parameters of the HMM given the set of output sequences. No tractable algorithm is known for solving this problem exactly, but a local maximum likelihood can be derived efficiently using the <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" title="Baum–Welch algorithm">Baum–Welch algorithm</a> or the Baldi–Chauvin algorithm. The <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" title="Baum–Welch algorithm">Baum–Welch algorithm</a> is a special case of the <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" title="Expectation-maximization algorithm" class="mw-redirect">expectation-maximization algorithm</a>.</p>
<h2><span class="mw-headline" id="Mathematical_description">Mathematical description</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=12" title="Edit section: Mathematical description">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="General_description">General description</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=13" title="Edit section: General description">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A basic, non-Bayesian hidden Markov model can be described as follows:</p>
<table>
<tbody><tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of states</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/b9ece18c950afbfa6b0fdbfa4ff731d3.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of observations</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\theta_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/039efd8c203d0437fea584f00c7f2864.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>emission parameter for an observation associated with state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\phi_{i=1 \dots N, j=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4111958c42f11803b47d7990b4489269.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>probability of transition from state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"> to state <img class="mwe-math-fallback-image-inline tex" alt="j" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/363b122c528f54df4a0446b6bab05515.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png">-dimensional vector, composed of <img class="mwe-math-fallback-image-inline tex" alt="\phi_{i,1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5413dd46b868024f3c5b2bbe6bab0172.png">; must sum to <img class="mwe-math-fallback-image-inline tex" alt="1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c4ca4238a0b923820dcc509a6f75849b.png">, the row of the matrix <img class="mwe-math-fallback-image-inline tex" alt="\phi_{i=1 \dots N, j=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4111958c42f11803b47d7990b4489269.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/24e0cb152e7f79b7acce97b8e50203b8.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>(hidden) state at time <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>observation at time <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="F(y|\theta)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/eed2f85364c589cd091635cceb4ee562.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>probability distribution of an observation, parametrized on <img class="mwe-math-fallback-image-inline tex" alt="\theta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/50d91f80cbb8feda1d10e167107ad1ff.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=2 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a29c4e01d6b4450e405f0265f3f48532.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Categorical}(\boldsymbol\phi_{x_{t-1}})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/02301b59a6de634a5cf0e6362c941416.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="F(\theta_{x_t})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4caec30d835b21b8a0b4a1e4579bd84e.png"></td>
</tr>
</tbody></table>
<p>Note that, in the above model (and also the one below), the prior distribution of the initial state <img class="mwe-math-fallback-image-inline tex" alt="x_1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f9a3b8e9e501458e8face47cae8826de.png"> is not specified. Typical learning models correspond to assuming a discrete uniform distribution over possible states (i.e. no particular prior distribution is assumed).</p>
<p>In a Bayesian setting, all parameters are associated with random variables, as follows:</p>
<table>
<tbody><tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N,T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/21002a065f857b236bdeb31eeb5903f9.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>as above</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\theta_{i=1 \dots N}, \phi_{i=1 \dots N, j=1 \dots N}, \boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e8be71b979cd3d64759b9e1bcd8095a5.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>as above</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=1 \dots T}, y_{t=1 \dots T}, F(y|\theta)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/05eb5d1fe4550ae6a17af8f7d0e06ff1.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>as above</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\alpha" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/bccfc7022dfb945174d9bcebad2297bb.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>shared hyperparameter for emission parameters</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>shared hyperparameter for transition parameters</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="H(\theta|\alpha)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c672b3e1cc5d115d594a9db34dae15dd.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>prior probability distribution of emission parameters, parametrized on <img class="mwe-math-fallback-image-inline tex" alt="\alpha" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/bccfc7022dfb945174d9bcebad2297bb.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\theta_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/039efd8c203d0437fea584f00c7f2864.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="H(\alpha)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/ebe122789ffe899bb9b54f009209c6f5.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Symmetric-Dirichlet}_N(\beta)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5672e1857c5c980f1c765a533865dd0e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=2 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a29c4e01d6b4450e405f0265f3f48532.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Categorical}(\boldsymbol\phi_{x_{t-1}})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/02301b59a6de634a5cf0e6362c941416.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="F(\theta_{x_t})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4caec30d835b21b8a0b4a1e4579bd84e.png"></td>
</tr>
</tbody></table>
<p>These characterizations use <img class="mwe-math-fallback-image-inline tex" alt="F" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/800618943025315f869e4e1f09471012.png"> and <img class="mwe-math-fallback-image-inline tex" alt="H" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c1d9f50f86825a1a2302ec2449c17196.png"> to describe arbitrary distributions over observations and parameters, respectively. Typically <img class="mwe-math-fallback-image-inline tex" alt="H" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c1d9f50f86825a1a2302ec2449c17196.png"> will be the <a href="https://en.wikipedia.org/wiki/Conjugate_prior" title="Conjugate prior">conjugate prior</a> of <img class="mwe-math-fallback-image-inline tex" alt="F" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/800618943025315f869e4e1f09471012.png">. The two most common choices of <img class="mwe-math-fallback-image-inline tex" alt="F" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/800618943025315f869e4e1f09471012.png"> are <a href="https://en.wikipedia.org/wiki/Gaussian_distribution" title="Gaussian distribution" class="mw-redirect">Gaussian</a> and <a href="https://en.wikipedia.org/wiki/Categorical_distribution" title="Categorical distribution">categorical</a>; see below.</p>
<h3><span class="mw-headline" id="Compared_with_a_simple_mixture_model">Compared with a simple mixture model</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=14" title="Edit section: Compared with a simple mixture model">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>As mentioned above, the distribution of each observation in a hidden Markov model is a <a href="https://en.wikipedia.org/wiki/Mixture_density" title="Mixture density" class="mw-redirect">mixture density</a>, with the states of the corresponding to mixture components. It is useful to compare the above characterizations for an HMM with the corresponding characterizations, of a <a href="https://en.wikipedia.org/wiki/Mixture_model" title="Mixture model">mixture model</a>, using the same notation.</p>
<p>A non-Bayesian mixture model:</p>
<table>
<tbody><tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of mixture components</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/b9ece18c950afbfa6b0fdbfa4ff731d3.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of observations</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\theta_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/039efd8c203d0437fea584f00c7f2864.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>parameter of distribution of observation associated with component <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/13791f8507d418cf9b4a683ca2c390e3.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>mixture weight, i.e. prior probability of component <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/dd9c0b2d685d923192e00357b3d7f901.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png">-dimensional vector, composed of <img class="mwe-math-fallback-image-inline tex" alt="\phi_{1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f49eb18319610e91c4541e2f3cf2ec1a.png">; must sum to <img class="mwe-math-fallback-image-inline tex" alt="1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c4ca4238a0b923820dcc509a6f75849b.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/24e0cb152e7f79b7acce97b8e50203b8.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>component of observation <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>observation <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="F(y|\theta)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/eed2f85364c589cd091635cceb4ee562.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>probability distribution of an observation, parametrized on <img class="mwe-math-fallback-image-inline tex" alt="\theta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/50d91f80cbb8feda1d10e167107ad1ff.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/24e0cb152e7f79b7acce97b8e50203b8.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Categorical}(\boldsymbol\phi)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d3482a3bca26256c38768cae0d3565e2.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="F(\theta_{x_t})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4caec30d835b21b8a0b4a1e4579bd84e.png"></td>
</tr>
</tbody></table>
<p>A Bayesian mixture model:</p>
<table>
<tbody><tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N,T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/21002a065f857b236bdeb31eeb5903f9.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>as above</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\theta_{i=1 \dots N}, \phi_{i=1 \dots N}, \boldsymbol\phi" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5da26e242e29d57cd09e75f492df3ccc.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>as above</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=1 \dots T}, y_{t=1 \dots T}, F(y|\theta)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/05eb5d1fe4550ae6a17af8f7d0e06ff1.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>as above</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\alpha" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/bccfc7022dfb945174d9bcebad2297bb.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>shared hyperparameter for component parameters</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>shared hyperparameter for mixture weights</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="H(\theta|\alpha)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c672b3e1cc5d115d594a9db34dae15dd.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>prior probability distribution of component parameters, parametrized on <img class="mwe-math-fallback-image-inline tex" alt="\alpha" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/bccfc7022dfb945174d9bcebad2297bb.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\theta_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/039efd8c203d0437fea584f00c7f2864.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="H(\alpha)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/ebe122789ffe899bb9b54f009209c6f5.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/dd9c0b2d685d923192e00357b3d7f901.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Symmetric-Dirichlet}_N(\beta)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5672e1857c5c980f1c765a533865dd0e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/24e0cb152e7f79b7acce97b8e50203b8.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Categorical}(\boldsymbol\phi)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d3482a3bca26256c38768cae0d3565e2.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="F(\theta_{x_t})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4caec30d835b21b8a0b4a1e4579bd84e.png"></td>
</tr>
</tbody></table>
<h3><span class="mw-headline" id="Examples">Examples</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=15" title="Edit section: Examples">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The following mathematical descriptions are fully written out and explained, for ease of implementation.</p>
<p>A typical non-Bayesian HMM with Gaussian observations looks like this:</p>
<table>
<tbody><tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of states</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/b9ece18c950afbfa6b0fdbfa4ff731d3.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of observations</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\phi_{i=1 \dots N, j=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4111958c42f11803b47d7990b4489269.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>probability of transition from state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"> to state <img class="mwe-math-fallback-image-inline tex" alt="j" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/363b122c528f54df4a0446b6bab05515.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png">-dimensional vector, composed of <img class="mwe-math-fallback-image-inline tex" alt="\phi_{i,1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5413dd46b868024f3c5b2bbe6bab0172.png">; must sum to <img class="mwe-math-fallback-image-inline tex" alt="1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c4ca4238a0b923820dcc509a6f75849b.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\mu_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/1b7cb9dd0a2c5301b3bc985b64ace16c.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>mean of observations associated with state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sigma^2_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/cad3c7790dde0e622a30bf09825437e7.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>variance of observations associated with state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/24e0cb152e7f79b7acce97b8e50203b8.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>state of observation at time <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>observation at time <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=2 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a29c4e01d6b4450e405f0265f3f48532.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Categorical}(\boldsymbol\phi_{x_{t-1}})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/02301b59a6de634a5cf0e6362c941416.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\mathcal{N}(\mu_{x_t}, \sigma_{x_t}^2)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/79abca8455fac571e47e9c85d0deedc0.png"></td>
</tr>
</tbody></table>
<p>A typical Bayesian HMM with Gaussian observations looks like this:</p>
<table>
<tbody><tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of states</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/b9ece18c950afbfa6b0fdbfa4ff731d3.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of observations</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\phi_{i=1 \dots N, j=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4111958c42f11803b47d7990b4489269.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>probability of transition from state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"> to state <img class="mwe-math-fallback-image-inline tex" alt="j" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/363b122c528f54df4a0446b6bab05515.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png">-dimensional vector, composed of <img class="mwe-math-fallback-image-inline tex" alt="\phi_{i,1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5413dd46b868024f3c5b2bbe6bab0172.png">; must sum to <img class="mwe-math-fallback-image-inline tex" alt="1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c4ca4238a0b923820dcc509a6f75849b.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\mu_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/1b7cb9dd0a2c5301b3bc985b64ace16c.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>mean of observations associated with state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sigma^2_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/cad3c7790dde0e622a30bf09825437e7.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>variance of observations associated with state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/24e0cb152e7f79b7acce97b8e50203b8.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>state of observation at time <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>observation at time <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>concentration hyperparameter controlling the density of the transition matrix</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\mu_0, \lambda" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/847614b329c73a46a77e9e87cf15148d.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>shared hyperparameters of the means for each state</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\nu, \sigma_0^2" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/61d04d6e670a7ff6208bacaab0f238f9.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>shared hyperparameters of the variances for each state</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Symmetric-Dirichlet}_N(\beta)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5672e1857c5c980f1c765a533865dd0e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=2 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a29c4e01d6b4450e405f0265f3f48532.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Categorical}(\boldsymbol\phi_{x_{t-1}})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/02301b59a6de634a5cf0e6362c941416.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\mu_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/1b7cb9dd0a2c5301b3bc985b64ace16c.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\mathcal{N}(\mu_0, \lambda\sigma_i^2)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/1abb48a40b0e3527d9329e125c887cef.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sigma_{i=1 \dots N}^2" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/cad3c7790dde0e622a30bf09825437e7.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Inverse-Gamma}(\nu, \sigma_0^2)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f5d66f0f3056ff091ea5689966bc498a.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1  \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\mathcal{N}(\mu_{x_t}, \sigma_{x_t}^2)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/79abca8455fac571e47e9c85d0deedc0.png"></td>
</tr>
</tbody></table>
<p>A typical non-Bayesian HMM with categorical observations looks like this:</p>
<table>
<tbody><tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of states</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/b9ece18c950afbfa6b0fdbfa4ff731d3.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of observations</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\phi_{i=1 \dots N, j=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4111958c42f11803b47d7990b4489269.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>probability of transition from state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"> to state <img class="mwe-math-fallback-image-inline tex" alt="j" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/363b122c528f54df4a0446b6bab05515.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png">-dimensional vector, composed of <img class="mwe-math-fallback-image-inline tex" alt="\phi_{i,1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5413dd46b868024f3c5b2bbe6bab0172.png">; must sum to <img class="mwe-math-fallback-image-inline tex" alt="1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c4ca4238a0b923820dcc509a6f75849b.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="V" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5206560a306a2e085a437fd258eb57ce.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>dimension of categorical observations, e.g. size of word vocabulary</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\theta_{i=1 \dots N, j=1 \dots V}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a548e34dd41388329bce4e882bdb937e.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>probability for state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"> of observing the <img class="mwe-math-fallback-image-inline tex" alt="j" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/363b122c528f54df4a0446b6bab05515.png">th item</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\theta_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c830ec313f6ad6a0dffe3458b93a6e0f.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="V" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5206560a306a2e085a437fd258eb57ce.png">-dimensional vector, composed of <img class="mwe-math-fallback-image-inline tex" alt="\theta_{i,1 \dots V}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/17a31998391531871447c258a6fb175a.png">; must sum to <img class="mwe-math-fallback-image-inline tex" alt="1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c4ca4238a0b923820dcc509a6f75849b.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/24e0cb152e7f79b7acce97b8e50203b8.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>state of observation at time <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>observation at time <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=2 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a29c4e01d6b4450e405f0265f3f48532.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Categorical}(\boldsymbol\phi_{x_{t-1}})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/02301b59a6de634a5cf0e6362c941416.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\text{Categorical}(\boldsymbol\theta_{x_t})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/299bf72a1e9a3ad906013eb2a8f27f50.png"></td>
</tr>
</tbody></table>
<p>A typical Bayesian HMM with categorical observations looks like this:</p>
<table>
<tbody><tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of states</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/b9ece18c950afbfa6b0fdbfa4ff731d3.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>number of observations</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\phi_{i=1 \dots N, j=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4111958c42f11803b47d7990b4489269.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>probability of transition from state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"> to state <img class="mwe-math-fallback-image-inline tex" alt="j" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/363b122c528f54df4a0446b6bab05515.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png">-dimensional vector, composed of <img class="mwe-math-fallback-image-inline tex" alt="\phi_{i,1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5413dd46b868024f3c5b2bbe6bab0172.png">; must sum to <img class="mwe-math-fallback-image-inline tex" alt="1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c4ca4238a0b923820dcc509a6f75849b.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="V" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5206560a306a2e085a437fd258eb57ce.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>dimension of categorical observations, e.g. size of word vocabulary</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\theta_{i=1 \dots N, j=1 \dots V}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a548e34dd41388329bce4e882bdb937e.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>probability for state <img class="mwe-math-fallback-image-inline tex" alt="i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/865c0c0b4ab0e063e5caa3387c1a8741.png"> of observing the <img class="mwe-math-fallback-image-inline tex" alt="j" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/363b122c528f54df4a0446b6bab05515.png">th item</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\theta_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c830ec313f6ad6a0dffe3458b93a6e0f.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="V" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5206560a306a2e085a437fd258eb57ce.png">-dimensional vector, composed of <img class="mwe-math-fallback-image-inline tex" alt="\theta_{i,1 \dots V}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/17a31998391531871447c258a6fb175a.png">; must sum to <img class="mwe-math-fallback-image-inline tex" alt="1" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c4ca4238a0b923820dcc509a6f75849b.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/24e0cb152e7f79b7acce97b8e50203b8.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>state of observation at time <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>observation at time <img class="mwe-math-fallback-image-inline tex" alt="t" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/e358efa489f58062f10dd7316b65649e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\alpha" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/bccfc7022dfb945174d9bcebad2297bb.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>shared concentration hyperparameter of <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\theta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f371c7df934e1fa0f81fb845eb819600.png"> for each state</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>concentration hyperparameter controlling the density of the transition matrix</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Symmetric-Dirichlet}_N(\beta)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5672e1857c5c980f1c765a533865dd0e.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\theta_{1 \dots V}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/abaa418189554395df3190a18791a484.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Symmetric-Dirichlet}_V(\alpha)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/7718f16099c39e953e83054c438ed7e3.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="x_{t=2 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a29c4e01d6b4450e405f0265f3f48532.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Categorical}(\boldsymbol\phi_{x_{t-1}})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/02301b59a6de634a5cf0e6362c941416.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="y_{t=1 \dots T}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/d0592f792af507d48c5c96f831a39207.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Categorical}(\boldsymbol\theta_{x_t})" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/45c23892aac1eb40e84f54af44ee434b.png"></td>
</tr>
</tbody></table>
<p>Note that in the above Bayesian characterizations, <img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"> (a <a href="https://en.wikipedia.org/wiki/Concentration_parameter" title="Concentration parameter">concentration parameter</a>) controls the density of the transition matrix. That is, with a high value of <img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"> (significantly above 1), the probabilities controlling the transition out of a particular state will all be similar, meaning there will be a significant probability of transitioning to any of the other states. In other words, the path followed by the Markov chain of hidden states will be highly random. With a low value of <img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"> (significantly below 1), only a small number of the possible transitions out of a given state will have significant probability, meaning that the path followed by the hidden states will be somewhat predictable.</p>
<h3><span class="mw-headline" id="A_two-level_Bayesian_HMM">A two-level Bayesian HMM</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=16" title="Edit section: A two-level Bayesian HMM">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An alternative for the above two Bayesian examples would be to add another level of prior parameters for the transition matrix. That is, replace the lines</p>
<table>
<tbody><tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>concentration hyperparameter controlling the density of the transition matrix</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Symmetric-Dirichlet}_N(\beta)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/5672e1857c5c980f1c765a533865dd0e.png"></td>
</tr>
</tbody></table>
<p>with the following:</p>
<table>
<tbody><tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\gamma" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/334de1ea38b615839e4ee6b65ee1b103.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>concentration hyperparameter controlling how many states are intrinsically likely</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td>concentration hyperparameter controlling the density of the transition matrix</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="=" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/43ec3e5dee6e706af7766fffea512721.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png">-dimensional vector of probabilities, specifying the intrinsic probability of a given state</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Symmetric-Dirichlet}_N(\gamma)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/76c403c76b0bb675136619ab1ec864d9.png"></td>
</tr>
<tr>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\sim" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/f55d4435e31a3e1d665905db4b6afe24.png"></td>
<td>&nbsp;</td>
<td><img class="mwe-math-fallback-image-inline tex" alt="\operatorname{Dirichlet}_N(\beta N \boldsymbol\eta)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/4387171f06ec3fc4d242d3f2cdeb2f76.png"></td>
</tr>
</tbody></table>
<p>What this means is the following:</p>
<ol>
<li><img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png"> is a <a href="https://en.wikipedia.org/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> over states, specifying which states are inherently likely. The greater the probability of a given state in this vector, the more likely is a transition to that state (regardless of the starting state).</li>
<li><img class="mwe-math-fallback-image-inline tex" alt="\gamma" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/334de1ea38b615839e4ee6b65ee1b103.png"> controls the density of <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png">. Values significantly above 1 cause a dense vector where all states will have similar <a href="https://en.wikipedia.org/wiki/Prior_probability" title="Prior probability">prior probabilities</a>. Values significantly below 1 cause a sparse vector where only a few states are inherently likely (have prior probabilities significantly above 0).</li>
<li><img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"> controls the density of the transition matrix, or more specifically, the density of the <i>N</i> different probability vectors <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi_{i=1 \dots N}" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/2c7310e9f34ab86a194683da9951faed.png"> specifying the probability of transitions out of state <i>i</i> to any other state.</li>
</ol>
<p>Imagine that the value of <img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"> is significantly above 1. Then the different <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/dd9c0b2d685d923192e00357b3d7f901.png"> vectors will be dense, i.e. the probability mass will be spread out fairly evenly over all states. However, to the extent that this mass is unevenly spread, <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png"> controls which states are likely to get more mass than others.</p>
<p>Now, imagine instead that <img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"> is significantly below 1. This will make the <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/dd9c0b2d685d923192e00357b3d7f901.png"> vectors sparse, i.e. almost all the probability mass is distributed over a small number of states, and for the rest, a transition to that state will be very unlikely. Notice that there are different <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/dd9c0b2d685d923192e00357b3d7f901.png"> vectors for each starting state, and so even if all the vectors are sparse, different vectors may distribute the mass to different ending states. However, for all of the vectors, <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png"> controls which ending states are likely to get mass assigned to them. For example, if <img class="mwe-math-fallback-image-inline tex" alt="\beta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/071997f13634882f823041b057f90923.png"> is 0.1, then each <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\phi" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/dd9c0b2d685d923192e00357b3d7f901.png"> will be sparse and, for any given starting state <i>i</i>, the set of states <img class="mwe-math-fallback-image-inline tex" alt="\mathbf{J}_i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c43f4905fcf052ad7811b6ceed1ab6c5.png"> to which transitions are likely to occur will be very small, typically having only one or two members. Now, if the probabilities in <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png"> are all the same (or equivalently, one of the above models without <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png"> is used), then for different <i>i</i>, there will be different states in the corresponding <img class="mwe-math-fallback-image-inline tex" alt="\mathbf{J}_i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c43f4905fcf052ad7811b6ceed1ab6c5.png">, so that all states are equally likely to occur in any given <img class="mwe-math-fallback-image-inline tex" alt="\mathbf{J}_i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c43f4905fcf052ad7811b6ceed1ab6c5.png">. On the other hand, if the values in <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png"> are unbalanced, so that one state has a much higher probability than others, almost all <img class="mwe-math-fallback-image-inline tex" alt="\mathbf{J}_i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/c43f4905fcf052ad7811b6ceed1ab6c5.png"> will contain this state; hence, regardless of the starting state, transitions will nearly always occur to this given state.</p>
<p>Hence, a two-level model such as just described allows independent control over (1) the overall density of the transition matrix, and (2) the density of states to which transitions are likely (i.e. the density of the prior distribution of states in any particular hidden variable <img class="mwe-math-fallback-image-inline tex" alt="x_i" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/05e42209d67fe1eb15a055e9d3b3770e.png">). In both cases this is done while still assuming ignorance over which particular states are more likely than others. If it is desired to inject this information into the model, the probability vector <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png"> can be directly specified; or, if there is less certainty about these relative probabilities, a non-symmetric <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution" title="Dirichlet distribution">Dirichlet distribution</a> can be used as the prior distribution over <img class="mwe-math-fallback-image-inline tex" alt="\boldsymbol\eta" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/42a8df66243f0135b54a0283e6f3a2e5.png">. That is, instead of using a symmetric Dirichlet distribution with the single parameter <img class="mwe-math-fallback-image-inline tex" alt="\gamma" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/334de1ea38b615839e4ee6b65ee1b103.png"> (or equivalently, a general Dirichlet with a vector all of whose values are equal to <img class="mwe-math-fallback-image-inline tex" alt="\gamma" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/334de1ea38b615839e4ee6b65ee1b103.png">), use a general Dirichlet with values that are variously greater or less than <img class="mwe-math-fallback-image-inline tex" alt="\gamma" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/334de1ea38b615839e4ee6b65ee1b103.png">, according to which state is more or less preferred.</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=17" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>HMMs can be applied in many fields where the goal is to recover a data sequence that is not immediately observable (but other data that depend on the sequence are). Applications include:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Single-molecule_experiment" title="Single-molecule experiment">Single Molecule Kinetic analysis</a><sup id="cite_ref-16" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-16">[16]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/Cryptanalysis" title="Cryptanalysis">Cryptanalysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a></li>
<li><a href="https://en.wikipedia.org/wiki/Speech_synthesis" title="Speech synthesis">Speech synthesis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" title="Part-of-speech tagging">Part-of-speech tagging</a></li>
<li>Document Separation in scanning solutions</li>
<li><a href="https://en.wikipedia.org/wiki/Machine_translation" title="Machine translation">Machine translation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Partial_discharge" title="Partial discharge">Partial discharge</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gene_prediction" title="Gene prediction">Gene prediction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sequence_alignment" title="Sequence alignment">Alignment of bio-sequences</a></li>
<li><a href="https://en.wikipedia.org/wiki/Time_series" title="Time series">Time Series Analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Activity_recognition" title="Activity recognition">Activity recognition</a></li>
<li><a href="https://en.wikipedia.org/wiki/Protein_folding" title="Protein folding">Protein folding</a><sup id="cite_ref-17" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-17">[17]</a></sup></li>
<li>Metamorphic Virus Detection<sup id="cite_ref-18" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-18">[18]</a></sup></li>
<li>DNA Motif Discovery<sup id="cite_ref-19" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-19">[19]</a></sup></li>
</ul>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=18" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The forward and backward recursions used in HMM as well as computations of marginal smoothing probabilities were first described by <a href="https://en.wikipedia.org/wiki/Ruslan_L._Stratonovich" title="Ruslan L. Stratonovich" class="mw-redirect">Ruslan L. Stratonovich</a> in 1960<sup id="cite_ref-Stratonovich1960_6-1" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-Stratonovich1960-6">[6]</a></sup> (pages 160—162) and in the late 1950s in his papers in Russian. The Hidden Markov Models were later described in a series of statistical papers by <a href="https://en.wikipedia.org/wiki/Leonard_E._Baum" title="Leonard E. Baum">Leonard E. Baum</a> and other authors in the second half of the 1960s. One of the first applications of HMMs was <a href="https://en.wikipedia.org/wiki/Speech_recognition" title="Speech recognition">speech recognition</a>, starting in the mid-1970s.<sup id="cite_ref-20" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-20">[20]</a></sup><sup id="cite_ref-21" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-21">[21]</a></sup><sup id="cite_ref-22" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-22">[22]</a></sup><sup id="cite_ref-23" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-23">[23]</a></sup></p>
<p>In the second half of the 1980s, HMMs began to be applied to the analysis of biological sequences,<sup id="cite_ref-24" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-24">[24]</a></sup> in particular <a href="https://en.wikipedia.org/wiki/DNA" title="DNA">DNA</a>. Since then, they have become ubiquitous in the field of <a href="https://en.wikipedia.org/wiki/Bioinformatics" title="Bioinformatics">bioinformatics</a>.<sup id="cite_ref-25" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-25">[25]</a></sup></p>
<h2><span class="mw-headline" id="Types">Types</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=19" title="Edit section: Types">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Hidden Markov models can model complex <a href="https://en.wikipedia.org/wiki/Markov" title="Markov">Markov</a> processes where the states emit the observations according to some probability distribution. One such example is the <a href="https://en.wikipedia.org/wiki/Gaussian" title="Gaussian" class="mw-redirect">Gaussian</a> distribution, in such a Hidden Markov Model the states output are represented by a <a href="https://en.wikipedia.org/wiki/Gaussian" title="Gaussian" class="mw-redirect">Gaussian</a> distribution.</p>
<p>Moreover, it could represent even more complex behavior when the output of the states is represented as mixture of two or more Gaussians, in which case the <a href="https://en.wikipedia.org/wiki/Probability" title="Probability">probability</a> of generating an observation is the product of the probability of first selecting one of the Gaussians and the probability of generating that observation from that Gaussian.</p>
<h2><span class="mw-headline" id="Extensions">Extensions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=20" title="Edit section: Extensions">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In the hidden Markov models considered above, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a <a href="https://en.wikipedia.org/wiki/Categorical_distribution" title="Categorical distribution">categorical distribution</a>) or continuous (typically from a <a href="https://en.wikipedia.org/wiki/Gaussian_distribution" title="Gaussian distribution" class="mw-redirect">Gaussian distribution</a>). Hidden Markov models can also be generalized to allow continuous state spaces. Examples of such models are those where the Markov process over hidden variables is a <a href="https://en.wikipedia.org/wiki/Linear_dynamical_system" title="Linear dynamical system">linear dynamical system</a>, with a linear relationship among related variables and where all hidden and observed variables follow a <a href="https://en.wikipedia.org/wiki/Gaussian_distribution" title="Gaussian distribution" class="mw-redirect">Gaussian distribution</a>. In simple cases, such as the linear dynamical system just mentioned, exact inference is tractable (in this case, using the <a href="https://en.wikipedia.org/wiki/Kalman_filter" title="Kalman filter">Kalman filter</a>); however, in general, exact inference in HMMs with continuous latent variables is infeasible, and approximate methods must be used, such as the <a href="https://en.wikipedia.org/wiki/Extended_Kalman_filter" title="Extended Kalman filter">extended Kalman filter</a> or the <a href="https://en.wikipedia.org/wiki/Particle_filter" title="Particle filter">particle filter</a>.</p>
<p>Hidden Markov models are <a href="https://en.wikipedia.org/wiki/Generative_model" title="Generative model">generative models</a>, in which the <a href="https://en.wikipedia.org/wiki/Joint_distribution" title="Joint distribution" class="mw-redirect">joint distribution</a> of observations and hidden states, or equivalently both the <a href="https://en.wikipedia.org/wiki/Prior_distribution" title="Prior distribution" class="mw-redirect">prior distribution</a> of hidden states (the <i>transition probabilities</i>) and <a href="https://en.wikipedia.org/wiki/Conditional_distribution" title="Conditional distribution" class="mw-redirect">conditional distribution</a> of observations given states (the <i>emission probabilities</i>), is modeled. The above algorithms implicitly assume a <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)" title="Uniform distribution (continuous)">uniform</a> prior distribution over the transition probabilities. However, it is also possible to create hidden Markov models with other types of prior distributions. An obvious candidate, given the categorical distribution of the transition probabilities, is the <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution" title="Dirichlet distribution">Dirichlet distribution</a>, which is the <a href="https://en.wikipedia.org/wiki/Conjugate_prior" title="Conjugate prior">conjugate prior</a> distribution of the categorical distribution. Typically, a symmetric Dirichlet distribution is chosen, reflecting ignorance about which states are inherently more likely than others. The single parameter of this distribution (termed the <i>concentration parameter</i>) controls the relative density or sparseness of the resulting transition matrix. A choice of 1 yields a uniform distribution. Values greater than 1 produce a dense matrix, in which the transition probabilities between pairs of states are likely to be nearly equal. Values less than 1 result in a sparse matrix in which, for each given source state, only a small number of destination states have non-negligible transition probabilities. It is also possible to use a two-level prior Dirichlet distribution, in which one Dirichlet distribution (the upper distribution) governs the parameters of another Dirichlet distribution (the lower distribution), which in turn governs the transition probabilities. The upper distribution governs the overall distribution of states, determining how likely each state is to occur; its concentration parameter determines the density or sparseness of states. Such a two-level prior distribution, where both concentration parameters are set to produce sparse distributions, might be useful for example in <a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" title="Part-of-speech tagging">part-of-speech tagging</a>, where some parts of speech occur much more commonly than others; learning algorithms that assume a uniform prior distribution generally perform poorly on this task. The parameters of models of this sort, with non-uniform prior distributions, can be learned using <a href="https://en.wikipedia.org/wiki/Gibbs_sampling" title="Gibbs sampling">Gibbs sampling</a> or extended versions of the <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" title="Expectation-maximization algorithm" class="mw-redirect">expectation-maximization algorithm</a>.</p>
<p>An extension of the previously described hidden Markov models with <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution" title="Dirichlet distribution">Dirichlet</a> priors uses a <a href="https://en.wikipedia.org/wiki/Dirichlet_process" title="Dirichlet process">Dirichlet process</a> in place of a Dirichlet distribution. This type of model allows for an unknown and potentially infinite number of states. It is common to use a two-level Dirichlet process, similar to the previously described model with two levels of Dirichlet distributions. Such a model is called a <i>hierarchical Dirichlet process hidden Markov model</i>, or <i>HDP-HMM</i> for short. It was originally described under the name "Infinite Hidden Markov Model"<sup class="reference plainlinks nourlexpansion" id=".22ref_Beal.2C"><a class="external autonumber" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#endnote_Beal.2C_Matthew_J..2C_Zoubin_Ghahramani.2C_and_Carl_Edward_Rasmussen._.22The_infinite_hidden_Markov_model..22_Advances_in_neural_information_processing_systems_14_.282002.29:_577-584.">[2]</a></sup> and was further formalized in<sup class="reference plainlinks nourlexpansion" id=".22ref_Teh.2C"><a class="external autonumber" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#endnote_Teh.2C_Yee_Whye.2C_et_al._.22Hierarchical_dirichlet_processes..22_Journal_of_the_American_Statistical_Association_101.476_.282006.29.">[3]</a></sup>.</p>
<p>A different type of extension uses a <a href="https://en.wikipedia.org/wiki/Discriminative_model" title="Discriminative model">discriminative model</a> in place of the <a href="https://en.wikipedia.org/wiki/Generative_model" title="Generative model">generative model</a> of standard HMMs. This type of model directly models the conditional distribution of the hidden states given the observations, rather than modeling the joint distribution. An example of this model is the so-called <i><a href="https://en.wikipedia.org/wiki/Maximum_entropy_Markov_model" title="Maximum entropy Markov model" class="mw-redirect">maximum entropy Markov model</a></i> (MEMM), which models the conditional distribution of the states using <a href="https://en.wikipedia.org/wiki/Logistic_regression" title="Logistic regression">logistic regression</a> (also known as a "<a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution" title="Maximum entropy probability distribution">maximum entropy</a> model"). The advantage of this type of model is that arbitrary features (i.e. functions) of the observations can be modeled, allowing domain-specific knowledge of the problem at hand to be injected into the model. Models of this sort are not limited to modeling direct dependencies between a hidden state and its associated observation; rather, features of nearby observations, of combinations of the associated observation and nearby observations, or in fact of arbitrary observations at any distance from a given hidden state can be included in the process used to determine the value of a hidden state. Furthermore, there is no need for these features to be <a href="https://en.wikipedia.org/wiki/Statistically_independent" title="Statistically independent" class="mw-redirect">statistically independent</a> of each other, as would be the case if such features were used in a generative model. Finally, arbitrary features over pairs of adjacent hidden states can be used rather than simple transition probabilities. The disadvantages of such models are: (1) The types of prior distributions that can be placed on hidden states are severely limited; (2) It is not possible to predict the probability of seeing an arbitrary observation. This second limitation is often not an issue in practice, since many common usages of HMM's do not require such predictive probabilities.</p>
<p>A variant of the previously described discriminative model is the linear-chain <a href="https://en.wikipedia.org/wiki/Conditional_random_field" title="Conditional random field">conditional random field</a>. This uses an undirected graphical model (aka <a href="https://en.wikipedia.org/wiki/Markov_random_field" title="Markov random field">Markov random field</a>) rather than the directed graphical models of MEMM's and similar models. The advantage of this type of model is that it does not suffer from the so-called <i>label bias</i> problem of MEMM's, and thus may make more accurate predictions. The disadvantage is that training can be slower than for MEMM's.</p>
<p>Yet another variant is the <i>factorial hidden Markov model</i>, which allows for a single observation to be conditioned on the corresponding hidden variables of a set of <img class="mwe-math-fallback-image-inline tex" alt="K" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a5f3c6a11b03839d46af9fb43c97c188.png"> independent Markov chains, rather than a single Markov chain. It is equivalent to a single HMM, with <img class="mwe-math-fallback-image-inline tex" alt="N^K" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/bd54000216f9e506bc78df0aec94e53c.png"> states (assuming there are <img class="mwe-math-fallback-image-inline tex" alt="N" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/8d9c307cb7f3c4a32822a51922d1ceaa.png"> states for each chain), and therefore, learning in such a model is difficult: for a sequence of length <img class="mwe-math-fallback-image-inline tex" alt="T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/b9ece18c950afbfa6b0fdbfa4ff731d3.png">, a straightforward Viterbi algorithm has complexity <img class="mwe-math-fallback-image-inline tex" alt="O(N^{2K} \, T)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/bf40a537ed9e8016903a22418b84806d.png">. To find an exact solution, a junction tree algorithm could be used, but it results in an <img class="mwe-math-fallback-image-inline tex" alt="O(N^{K+1} \, K \, T)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a108de954e78b7ea44d69bbe1f46053d.png"> complexity. In practice, approximate techniques, such as variational approaches, could be used.<sup id="cite_ref-26" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-26">[26]</a></sup></p>
<p>All of the above models can be extended to allow for more distant dependencies among hidden states, e.g. allowing for a given state to be dependent on the previous two or three states rather than a single previous state; i.e. the transition probabilities are extended to encompass sets of three or four adjacent states (or in general <img class="mwe-math-fallback-image-inline tex" alt="K" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a5f3c6a11b03839d46af9fb43c97c188.png"> adjacent states). The disadvantage of such models is that dynamic-programming algorithms for training them have an <img class="mwe-math-fallback-image-inline tex" alt="O(N^K \, T)" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/0b4f7b2d615eb381d0250c1b8fc8f05b.png"> running time, for <img class="mwe-math-fallback-image-inline tex" alt="K" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/a5f3c6a11b03839d46af9fb43c97c188.png"> adjacent states and <img class="mwe-math-fallback-image-inline tex" alt="T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/b9ece18c950afbfa6b0fdbfa4ff731d3.png"> total observations (i.e. a length-<img class="mwe-math-fallback-image-inline tex" alt="T" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/b9ece18c950afbfa6b0fdbfa4ff731d3.png"> Markov chain).</p>
<p>Another recent extension is the <i>triplet Markov model</i>,<sup id="cite_ref-TMM_27-0" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-TMM-27">[27]</a></sup> in which an auxiliary underlying process is added to model some data specificities. Many variants of this model have been proposed. One should also mention the interesting link that has been established between the <i>theory of evidence</i> and the <i>triplet Markov models</i> <sup id="cite_ref-TMMEV_10-1" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-TMMEV-10">[10]</a></sup> and which allows to fuse data in Markovian context <sup id="cite_ref-JASP_11-1" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-JASP-11">[11]</a></sup> and to model nonstationary data.<sup id="cite_ref-TSP_12-1" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-TSP-12">[12]</a></sup><sup id="cite_ref-SPL_13-1" class="reference"><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_note-SPL-13">[13]</a></sup></p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=21" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="div-col columns column-count column-count-3" style="-moz-column-count: 3; -webkit-column-count: 3; column-count: 3;">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Andrey_Markov" title="Andrey Markov">Andrey Markov</a></li>
<li><a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" title="Baum–Welch algorithm">Baum–Welch algorithm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bayesian_programming" title="Bayesian programming">Bayesian programming</a></li>
<li><a href="https://en.wikipedia.org/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>
<li><a href="https://en.wikipedia.org/wiki/Estimation_theory" title="Estimation theory">Estimation theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/HHpred_/_HHsearch" title="HHpred / HHsearch">HHpred / HHsearch</a> free server and software for protein sequence searching</li>
<li><a href="https://en.wikipedia.org/wiki/HMMER" title="HMMER">HMMER</a>, a free hidden Markov model program for protein sequence analysis</li>
<li><a href="https://en.wikipedia.org/wiki/Hidden_Bernoulli_model" title="Hidden Bernoulli model" class="mw-redirect">Hidden Bernoulli model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hidden_semi-Markov_model" title="Hidden semi-Markov model">Hidden semi-Markov model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hierarchical_hidden_Markov_model" title="Hierarchical hidden Markov model">Hierarchical hidden Markov model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Layered_hidden_Markov_model" title="Layered hidden Markov model">Layered hidden Markov model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Poisson_hidden_Markov_model" title="Poisson hidden Markov model">Poisson hidden Markov model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sequential_dynamical_system" title="Sequential dynamical system">Sequential dynamical system</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stochastic_context-free_grammar" title="Stochastic context-free grammar">Stochastic context-free grammar</a></li>
<li><a href="https://en.wikipedia.org/wiki/Time_Series" title="Time Series" class="mw-redirect">Time Series</a> Analysis</li>
<li><a href="https://en.wikipedia.org/wiki/Variable-order_Markov_model" title="Variable-order Markov model">Variable-order Markov model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" title="Viterbi algorithm">Viterbi algorithm</a></li>
</ul>
</div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=22" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-1"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Baum, L. E.; Petrie, T. (1966). <a rel="nofollow" class="external text" href="http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?handle=euclid.aoms/1177699147&amp;view=body&amp;content-type=pdf_1">"Statistical Inference for Probabilistic Functions of Finite State Markov Chains"</a>. <i>The Annals of Mathematical Statistics</i> <b>37</b> (6): 1554–1563. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1214%2Faoms%2F1177699147">10.1214/aoms/1177699147</a><span class="reference-accessdate">. Retrieved <span class="nowrap">28 November</span> 2011</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=Statistical+Inference+for+Probabilistic+Functions+of+Finite+State+Markov+Chains&amp;rft.aufirst=L.+E.&amp;rft.aulast=Baum&amp;rft.au=Petrie%2C+T.&amp;rft.date=1966&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fprojecteuclid.org%2FDPubS%2FRepository%2F1.0%2FDisseminate%3Fhandle%3Deuclid.aoms%2F1177699147%26view%3Dbody%26content-type%3Dpdf_1&amp;rft_id=info%3Adoi%2F10.1214%2Faoms%2F1177699147&amp;rft.issue=6&amp;rft.jtitle=The+Annals+of+Mathematical+Statistics&amp;rft.pages=1554-1563&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=37" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-2"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Baum, L. E.; Eagon, J. A. (1967). <a rel="nofollow" class="external text" href="http://projecteuclid.org/euclid.bams/1183528841">"An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology"</a>. <i><a href="https://en.wikipedia.org/wiki/Bulletin_of_the_American_Mathematical_Society" title="Bulletin of the American Mathematical Society">Bulletin of the American Mathematical Society</a></i> <b>73</b> (3): 360. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1090%2FS0002-9904-1967-11751-8">10.1090/S0002-9904-1967-11751-8</a>. <a href="https://en.wikipedia.org/wiki/Zentralblatt_MATH" title="Zentralblatt MATH">Zbl</a>&nbsp;<a rel="nofollow" class="external text" href="https://zbmath.org/?format=complete&amp;q=an:0157.11101">0157.11101</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=An+inequality+with+applications+to+statistical+estimation+for+probabilistic+functions+of+Markov+processes+and+to+a+model+for+ecology&amp;rft.au=Eagon%2C+J.+A.&amp;rft.aufirst=L.+E.&amp;rft.aulast=Baum&amp;rft.date=1967&amp;rft.genre=article&amp;rft_id=%2F%2Fzbmath.org%2F%3Fformat%3Dcomplete%26q%3Dan%3A0157.11101&amp;rft_id=http%3A%2F%2Fprojecteuclid.org%2Feuclid.bams%2F1183528841&amp;rft_id=info%3Adoi%2F10.1090%2FS0002-9904-1967-11751-8&amp;rft.issue=3&amp;rft.jtitle=Bulletin+of+the+American+Mathematical+Society&amp;rft.pages=360&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=73" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-3"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Baum, L. E.; Sell, G. R. (1968). <a rel="nofollow" class="external text" href="http://www.scribd.com/doc/6369908/Growth-Functions-for-Transformations-on-Manifolds">"Growth transformations for functions on manifolds"</a>. <i>Pacific Journal of Mathematics</i> <b>27</b> (2): 211–227. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.2140%2Fpjm.1968.27.211">10.2140/pjm.1968.27.211</a><span class="reference-accessdate">. Retrieved <span class="nowrap">28 November</span> 2011</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=Growth+transformations+for+functions+on+manifolds&amp;rft.aufirst=L.+E.&amp;rft.aulast=Baum&amp;rft.au=Sell%2C+G.+R.&amp;rft.date=1968&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.scribd.com%2Fdoc%2F6369908%2FGrowth-Functions-for-Transformations-on-Manifolds&amp;rft_id=info%3Adoi%2F10.2140%2Fpjm.1968.27.211&amp;rft.issue=2&amp;rft.jtitle=Pacific+Journal+of+Mathematics&amp;rft.pages=211-227&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=27" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-4"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="https://en.wikipedia.org/wiki/Leonard_E._Baum" title="Leonard E. Baum">Baum, L. E.</a>; Petrie, T.; Soules, G.; Weiss, N. (1970). <a rel="nofollow" class="external text" href="http://projecteuclid.org/euclid.aoms/1177697196">"A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains"</a>. <i><a href="https://en.wikipedia.org/wiki/The_Annals_of_Mathematical_Statistics" title="The Annals of Mathematical Statistics" class="mw-redirect">The Annals of Mathematical Statistics</a></i> <b>41</b>: 164. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1214%2Faoms%2F1177697196">10.1214/aoms/1177697196</a>. <a href="https://en.wikipedia.org/wiki/JSTOR" title="JSTOR">JSTOR</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.jstor.org/stable/2239727">2239727</a>. <a href="https://en.wikipedia.org/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ams.org/mathscinet-getitem?mr=MR287613">MR287613</a>. <a href="https://en.wikipedia.org/wiki/Zentralblatt_MATH" title="Zentralblatt MATH">Zbl</a>&nbsp;<a rel="nofollow" class="external text" href="https://zbmath.org/?format=complete&amp;q=an:0188.49603">0188.49603</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=A+Maximization+Technique+Occurring+in+the+Statistical+Analysis+of+Probabilistic+Functions+of+Markov+Chains&amp;rft.aufirst=L.+E.&amp;rft.aulast=Baum&amp;rft.au=Petrie%2C+T.&amp;rft.au=Soules%2C+G.&amp;rft.au=Weiss%2C+N.&amp;rft.date=1970&amp;rft.genre=article&amp;rft_id=%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3DMR287613&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2239727&amp;rft_id=%2F%2Fzbmath.org%2F%3Fformat%3Dcomplete%26q%3Dan%3A0188.49603&amp;rft_id=http%3A%2F%2Fprojecteuclid.org%2Feuclid.aoms%2F1177697196&amp;rft_id=info%3Adoi%2F10.1214%2Faoms%2F1177697196&amp;rft.jtitle=The+Annals+of+Mathematical+Statistics&amp;rft.pages=164&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=41" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-5"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Baum, L.E. (1972). "An Inequality and Associated Maximization Technique in Statistical Estimation of Probabilistic Functions of a Markov Process". <i>Inequalities</i> <b>3</b>: 1–8.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=An+Inequality+and+Associated+Maximization+Technique+in+Statistical+Estimation+of+Probabilistic+Functions+of+a+Markov+Process&amp;rft.aufirst=L.E.&amp;rft.aulast=Baum&amp;rft.date=1972&amp;rft.genre=article&amp;rft.jtitle=Inequalities&amp;rft.pages=1-8&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=3" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-Stratonovich1960-6"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-Stratonovich1960_6-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-Stratonovich1960_6-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Stratonovich, R.L. (1960). "Conditional Markov Processes". <i>Theory of Probability and its Applications</i> <b>5</b> (2): 156–178. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1137%2F1105015">10.1137/1105015</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=Conditional+Markov+Processes&amp;rft.au=Stratonovich%2C+R.L.&amp;rft.date=1960&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1137%2F1105015&amp;rft.issue=2&amp;rft.jtitle=Theory+of+Probability+and+its+Applications&amp;rft.pages=156-178&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=5" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-7"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Thad Starner, Alex Pentland. <a rel="nofollow" class="external text" href="http://www.cc.gatech.edu/~thad/p/031_10_SL/real-time-asl-recognition-from%20video-using-hmm-ISCV95.pdf">Real-Time American Sign Language Visual Recognition From Video Using Hidden Markov Models</a>. Master's Thesis, MIT, Feb 1995, Program in Media Arts</span></li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-8"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">B. Pardo and W. Birmingham. <a rel="nofollow" class="external text" href="http://www.cs.northwestern.edu/~pardo/publications/pardo-birmingham-aaai-05.pdf">Modeling Form for On-line Following of Musical Performances</a>. AAAI-05 Proc., July 2005.</span></li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-9"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Satish L, Gururaj BI (April 2003). "<a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=212242">Use of hidden Markov models for partial discharge pattern classification</a>". <i>IEEE Transactions on Dielectrics and Electrical Insulation</i>.</span></li>
<li id="cite_note-TMMEV-10"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-TMMEV_10-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-TMMEV_10-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.sciencedirect.com/science/article/pii/S0888613X06000375">Pr. Pieczynski</a>, W. Pieczynski, Multisensor triplet Markov chains and theory of evidence, International Journal of Approximate Reasoning, Vol. 45, No. 1, pp. 1-16, 2007.</span></li>
<li id="cite_note-JASP-11"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-JASP_11-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-JASP_11-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://asp.eurasipjournals.com/content/pdf/1687-6180-2012-134.pdf">Boudaren et al.</a>, M. Y. Boudaren, E. Monfrini, W. Pieczynski, and A. Aissani, Dempster-Shafer fusion of multisensor signals in nonstationary Markovian context, EURASIP Journal on Advances in Signal Processing, No. 134, 2012.</span></li>
<li id="cite_note-TSP-12"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-TSP_12-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-TSP_12-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=1468502&amp;contentType=Journals+%26+Magazines&amp;searchField%3DSearch_All%26queryText%3Dlanchantin+pieczynski">Lanchantin et al.</a>, P. Lanchantin and W. Pieczynski, Unsupervised restoration of hidden non stationary Markov chain using evidential priors, IEEE Trans. on Signal Processing, Vol. 53, No. 8, pp. 3091-3098, 2005.</span></li>
<li id="cite_note-SPL-13"><span class="mw-cite-backlink">^ <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-SPL_13-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-SPL_13-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6244854&amp;contentType=Journals+%26+Magazines&amp;searchField%3DSearch_All%26queryText%3Dboudaren">Boudaren et al.</a>, M. Y. Boudaren, E. Monfrini, and W. Pieczynski, Unsupervised segmentation of random discrete data hidden with switching noise distributions, IEEE Signal Processing Letters, Vol. 19, No. 10, pp. 619-622, October 2012.</span></li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-14"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="https://en.wikipedia.org/wiki/Lawrence_Rabiner" title="Lawrence Rabiner">Lawrence R. Rabiner</a> (February 1989). <a rel="nofollow" class="external text" href="http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf">"A tutorial on Hidden Markov Models and selected applications in speech recognition"</a> <span style="font-size:85%;">(PDF)</span>. <i>Proceedings of the <a href="https://en.wikipedia.org/wiki/IEEE" title="IEEE" class="mw-redirect">IEEE</a></i> <b>77</b> (2): 257–286. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1109%2F5.18626">10.1109/5.18626</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=A+tutorial+on+Hidden+Markov+Models+and+selected+applications+in+speech+recognition&amp;rft.au=Lawrence+R.+Rabiner&amp;rft.date=1989-02&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.ece.ucsb.edu%2FFaculty%2FRabiner%2Fece259%2FReprints%2Ftutorial%2520on%2520hmm%2520and%2520applications.pdf&amp;rft_id=info%3Adoi%2F10.1109%2F5.18626&amp;rft.issue=2&amp;rft.jtitle=Proceedings+of+the+IEEE&amp;rft.pages=257-286&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=77" class="Z3988"><span style="display:none;">&nbsp;</span></span> <a rel="nofollow" class="external autonumber" href="http://www.cs.cornell.edu/courses/cs481/2004fa/rabiner.pdf">[1]</a></span></li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-15"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Newberg, L. (2009). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2722652">"Error statistics of hidden Markov model and hidden Boltzmann model results"</a>. <i>BMC Bioinformatics</i> <b>10</b>: 212. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1186%2F1471-2105-10-212">10.1186/1471-2105-10-212</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Central" title="PubMed Central">PMC</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2722652">2722652</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Identifier" title="PubMed Identifier" class="mw-redirect">PMID</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pubmed/19589158">19589158</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=Error+statistics+of+hidden+Markov+model+and+hidden+Boltzmann+model+results&amp;rft.aufirst=L.&amp;rft.aulast=Newberg&amp;rft.date=2009&amp;rft.genre=article&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2722652&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2722652&amp;rft_id=info%3Adoi%2F10.1186%2F1471-2105-10-212&amp;rft_id=info%3Apmid%2F19589158&amp;rft.jtitle=BMC+Bioinformatics&amp;rft.pages=212&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=10" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-16"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">NICOLAI, CHRISTOPHER (2013). "SOLVING ION CHANNEL KINETICS WITH THE QuB SOFTWARE". <i>Biophysical Reviews and Letters</i> <b>8</b> (3n04): 191–211. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1142%2FS1793048013300053">10.1142/S1793048013300053</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=SOLVING+ION+CHANNEL+KINETICS+WITH+THE+QuB+SOFTWARE&amp;rft.aufirst=CHRISTOPHER&amp;rft.aulast=NICOLAI&amp;rft.date=2013&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1142%2FS1793048013300053&amp;rft.issue=3n04&amp;rft.jtitle=Biophysical+Reviews+and+Letters&amp;rft.pages=191-211&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=8" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-17"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Stigler, J.; Ziegler, F.; Gieseke, A.; Gebhardt, J. C. M.; Rief, M. (2011). "The Complex Folding Network of Single Calmodulin Molecules". <i><a href="https://en.wikipedia.org/wiki/Science_(journal)" title="Science (journal)">Science</a></i> <b>334</b> (6055): 512–516. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1126%2Fscience.1207598">10.1126/science.1207598</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Identifier" title="PubMed Identifier" class="mw-redirect">PMID</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pubmed/22034433">22034433</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=The+Complex+Folding+Network+of+Single+Calmodulin+Molecules&amp;rft.aufirst=J.&amp;rft.au=Gebhardt%2C+J.+C.+M.&amp;rft.au=Gieseke%2C+A.&amp;rft.aulast=Stigler&amp;rft.au=Rief%2C+M.&amp;rft.au=Ziegler%2C+F.&amp;rft.date=2011&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.1207598&amp;rft_id=info%3Apmid%2F22034433&amp;rft.issue=6055&amp;rft.jtitle=Science&amp;rft.pages=512-516&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=334" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-18"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Wong, W.; Stamp, M. (2006). "Hunting for metamorphic engines". <i>Journal in Computer Virology</i> <b>2</b> (3): 211–229. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1007%2Fs11416-006-0028-7">10.1007/s11416-006-0028-7</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=Hunting+for+metamorphic+engines&amp;rft.aufirst=W.&amp;rft.aulast=Wong&amp;rft.au=Stamp%2C+M.&amp;rft.date=2006&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2Fs11416-006-0028-7&amp;rft.issue=3&amp;rft.jtitle=Journal+in+Computer+Virology&amp;rft.pages=211-229&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=2" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-19"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Wong, K. -C.; Chan, T. -M.; Peng, C.; Li, Y.; Zhang, Z. (2013). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3763557">"DNA motif elucidation using belief propagation"</a>. <i>Nucleic Acids Research</i> <b>41</b> (16): e153. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1093%2Fnar%2Fgkt574">10.1093/nar/gkt574</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Central" title="PubMed Central">PMC</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3763557">3763557</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Identifier" title="PubMed Identifier" class="mw-redirect">PMID</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pubmed/23814189">23814189</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=DNA+motif+elucidation+using+belief+propagation&amp;rft.au=Chan%2C+T.+-M.&amp;rft.aufirst=K.+-C.&amp;rft.aulast=Wong&amp;rft.au=Li%2C+Y.&amp;rft.au=Peng%2C+C.&amp;rft.au=Zhang%2C+Z.&amp;rft.date=2013&amp;rft.genre=article&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3763557&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3763557&amp;rft_id=info%3Adoi%2F10.1093%2Fnar%2Fgkt574&amp;rft_id=info%3Apmid%2F23814189&amp;rft.issue=16&amp;rft.jtitle=Nucleic+Acids+Research&amp;rft.pages=e153&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=41" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-20"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="https://en.wikipedia.org/wiki/James_K._Baker" title="James K. Baker">Baker, J.</a> (1975). "The DRAGON system—An overview". <i>IEEE Transactions on Acoustics, Speech, and Signal Processing</i> <b>23</b>: 24–29. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1109%2FTASSP.1975.1162650">10.1109/TASSP.1975.1162650</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=The+DRAGON+system%94An+overview&amp;rft.aufirst=J.&amp;rft.aulast=Baker&amp;rft.date=1975&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1109%2FTASSP.1975.1162650&amp;rft.jtitle=IEEE+Transactions+on+Acoustics%2C+Speech%2C+and+Signal+Processing&amp;rft.pages=24-29&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=23" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-21"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Jelinek, F.; Bahl, L.; Mercer, R. (1975). "Design of a linguistic statistical decoder for the recognition of continuous speech". <i><a href="https://en.wikipedia.org/wiki/IEEE_Transactions_on_Information_Theory" title="IEEE Transactions on Information Theory">IEEE Transactions on Information Theory</a></i> <b>21</b> (3): 250. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1109%2FTIT.1975.1055384">10.1109/TIT.1975.1055384</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=Design+of+a+linguistic+statistical+decoder+for+the+recognition+of+continuous+speech&amp;rft.au=Bahl%2C+L.&amp;rft.aufirst=F.&amp;rft.aulast=Jelinek&amp;rft.au=Mercer%2C+R.&amp;rft.date=1975&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1975.1055384&amp;rft.issue=3&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.pages=250&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=21" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-22"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="https://en.wikipedia.org/wiki/Xuedong_Huang" title="Xuedong Huang">Xuedong Huang</a>; M. Jack; Y. Ariki (1990). <i>Hidden Markov Models for Speech Recognition</i>. Edinburgh University Press. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-7486-0162-7" title="Special:BookSources/0-7486-0162-7">0-7486-0162-7</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.au=M.+Jack&amp;rft.au=Xuedong+Huang&amp;rft.au=Y.+Ariki&amp;rft.btitle=Hidden+Markov+Models+for+Speech+Recognition&amp;rft.date=1990&amp;rft.genre=book&amp;rft.isbn=0-7486-0162-7&amp;rft.pub=Edinburgh+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-23"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="https://en.wikipedia.org/wiki/Xuedong_Huang" title="Xuedong Huang">Xuedong Huang</a>; Alex Acero; Hsiao-Wuen Hon (2001). <i>Spoken Language Processing</i>. Prentice Hall. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-13-022616-5" title="Special:BookSources/0-13-022616-5">0-13-022616-5</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.au=Alex+Acero&amp;rft.au=Hsiao-Wuen+Hon&amp;rft.au=Xuedong+Huang&amp;rft.btitle=Spoken+Language+Processing&amp;rft.date=2001&amp;rft.genre=book&amp;rft.isbn=0-13-022616-5&amp;rft.pub=Prentice+Hall&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-24"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">M. Bishop and E. Thompson (1986). "Maximum Likelihood Alignment of DNA Sequences". <i>Journal of Molecular Biology</i> <b>190</b> (2): 159–165. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1016%2F0022-2836%2886%2990289-5">10.1016/0022-2836(86)90289-5</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Identifier" title="PubMed Identifier" class="mw-redirect">PMID</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pubmed/3641921">3641921</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=Maximum+Likelihood+Alignment+of+DNA+Sequences&amp;rft.au=M.+Bishop+and+E.+Thompson&amp;rft.date=1986&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1016%2F0022-2836%2886%2990289-5&amp;rft_id=info%3Apmid%2F3641921&amp;rft.issue=2&amp;rft.jtitle=Journal+of+Molecular+Biology&amp;rft.pages=159-165&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=190" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-25"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation book">Richard Durbin; Sean R. Eddy; <a href="https://en.wikipedia.org/wiki/Anders_Krogh" title="Anders Krogh">Anders Krogh</a>; Graeme Mitchison (1999). <i>Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids</i>. <a href="https://en.wikipedia.org/wiki/Cambridge_University_Press" title="Cambridge University Press">Cambridge University Press</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-521-62971-3" title="Special:BookSources/0-521-62971-3">0-521-62971-3</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.au=Anders+Krogh&amp;rft.au=Graeme+Mitchison&amp;rft.au=Richard+Durbin&amp;rft.au=Sean+R.+Eddy&amp;rft.btitle=Biological+Sequence+Analysis%3A+Probabilistic+Models+of+Proteins+and+Nucleic+Acids&amp;rft.date=1999&amp;rft.genre=book&amp;rft.isbn=0-521-62971-3&amp;rft.pub=Cambridge+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-26"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Ghahramani, Zoubin; Jordan, Michael I. (1997). "Factorial Hidden Markov Models". <i>Machine Learning</i> <b>29</b> (2/3): 245–273. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://dx.doi.org/10.1023%2FA%3A1007425814087">10.1023/A:1007425814087</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model&amp;rft.atitle=Factorial+Hidden+Markov+Models&amp;rft.aufirst=Zoubin&amp;rft.au=Jordan%2C+Michael+I.&amp;rft.aulast=Ghahramani&amp;rft.date=1997&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1023%2FA%3A1007425814087&amp;rft.issue=2%2F3&amp;rft.jtitle=Machine+Learning&amp;rft.pages=245-273&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=29" class="Z3988"><span style="display:none;">&nbsp;</span></span></span></li>
<li id="cite_note-TMM-27"><span class="mw-cite-backlink"><b><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#cite_ref-TMM_27-0"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.sciencedirect.com/science/article/pii/S1631073X02024627">Triplet Markov Chain</a>, W. Pieczynski,Chaînes de Markov Triplet, Triplet Markov Chains, Comptes Rendus de l’Académie des Sciences – Mathématique, Série I, Vol. 335, No. 3, pp. 275-278, 2002.</span></li>
</ol>
</div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=23" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><img alt="" src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/12px-Commons-logo.svg.png" width="12" height="16" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/18px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/24px-Commons-logo.svg.png 2x" data-file-width="1024" data-file-height="1376"> Media related to <a href="https://commons.wikimedia.org/wiki/Category:Hidden_Markov_Model" class="extiw" title="commons:Category:Hidden Markov Model">Hidden Markov Model</a> at Wikimedia Commons</p>
<h3><span class="mw-headline" id="Concepts">Concepts</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=24" title="Edit section: Concepts">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li>Teif V. B. and K. Rippe (2010) Statistical–mechanical lattice models for protein–DNA binding in chromatin. <i>J. Phys.: Condens. Matter</i>, <b>22</b>, 414105, <a href="https://dx.doi.org/10.1088/0953-8984/22/41/414105" class="extiw" title="doi:10.1088/0953-8984/22/41/414105">http://iopscience.iop.org/0953-8984/22/41/414105</a></li>
<li><a rel="nofollow" class="external text" href="http://www.cs.sjsu.edu/~stamp/RUA/HMM.pdf">A Revealing Introduction to Hidden Markov Models</a> by Mark Stamp, San Jose State University.</li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20120415032315/http://www.ee.washington.edu/research/guptalab/publications/EMbookChenGupta2010.pdf">Fitting HMM's with expectation-maximization – complete derivation</a></li>
<li><a rel="nofollow" class="external text" href="http://www.tristanfletcher.co.uk/SAR%20HMM.pdf">Switching Autoregressive Hidden Markov Model (SAR HMM)</a></li>
<li><a rel="nofollow" class="external text" href="http://www.comp.leeds.ac.uk/roger/HiddenMarkovModels/html_dev/main.html">A step-by-step tutorial on HMMs</a> <i>(University of Leeds)</i></li>
<li><a rel="nofollow" class="external text" href="http://www.cs.brown.edu/research/ai/dynamics/tutorial/Documents/HiddenMarkovModels.html">Hidden Markov Models</a> <i>(an exposition using basic mathematics)</i></li>
<li><a rel="nofollow" class="external text" href="http://jedlik.phy.bme.hu/~gerjanos/HMM/node2.html">Hidden Markov Models</a> <i>(by Narada Warakagoda)</i></li>
<li>Hidden Markov Models: Fundamentals and Applications <a rel="nofollow" class="external text" href="http://www.eecis.udel.edu/~lliao/cis841s06/hmmtutorialpart1.pdf">Part 1</a>, <a rel="nofollow" class="external text" href="http://www.eecis.udel.edu/~lliao/cis841s06/hmmtutorialpart2.pdf">Part 2</a> <i>(by V. Petrushin)</i></li>
<li>Lecture on a Spreadsheet by Jason Eisner, <a rel="nofollow" class="external text" href="http://videolectures.net/hltss2010_eisner_plm/video/2/">Video</a> and <a rel="nofollow" class="external text" href="http://www.cs.jhu.edu/~jason/papers/eisner.hmm.xls">interactive spreadsheet</a></li>
</ul>
<h3><span class="mw-headline" id="Software">Software</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit&amp;section=25" title="Edit section: Software">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li><a rel="nofollow" class="external text" href="http://www.cs.ubc.ca/~murphyk/Software/HMM/hmm.html">Hidden Markov Model (HMM) Toolbox for Matlab</a> <i>(by Kevin Murphy)</i></li>
<li><a rel="nofollow" class="external text" href="http://htk.eng.cam.ac.uk/">Hidden Markov Model Toolkit (HTK)</a> <i>(a portable toolkit for building and manipulating hidden Markov models)</i></li>
<li><a rel="nofollow" class="external text" href="http://cran.r-project.org/web/packages/HMM/index.html">Hidden Markov Model R-Package</a> to set up, apply and make inference with discrete time and discrete space Hidden Markov Models</li>
<li><a rel="nofollow" class="external text" href="http://birc.au.dk/~asand/hmmlib">HMMlib</a> <i>(an optimized library for work with general (discrete) hidden Markov models)</i></li>
<li><a rel="nofollow" class="external text" href="http://birc.au.dk/~asand/parredhmmlib">parredHMMlib</a> <i>(a parallel implementation of the forward algorithm and the Viterbi algorithm. Extremely fast for HMMs with small state spaces)</i></li>
<li><a rel="nofollow" class="external text" href="http://birc.au.dk/software/zipHMM">zipHMMlib</a> <i>(a library for general (discrete) hidden Markov models, exploiting repetitions in the input sequence to greatly speed up the forward algorithm. Implementation of the posterior decoding algorithm and the Viterbi algorithm are also provided.)</i></li>
<li><a rel="nofollow" class="external text" href="http://www.ghmm.org/">GHMM Library</a> <i>(home page of the GHMM Library project)</i></li>
<li><a rel="nofollow" class="external text" href="http://jahmm.googlecode.com/">Jahmm Java Library</a> <i>(general-purpose Java library)</i></li>
<li><a rel="nofollow" class="external text" href="http://www.kanungo.com/software/software.html">HMM and other statistical programs</a> <i>(Implementation in C by Tapas Kanungo)</i></li>
<li><a rel="nofollow" class="external text" href="http://hackage.haskell.org/cgi-bin/hackage-scripts/package/hmm">The hmm package</a> A <a rel="nofollow" class="external text" href="http://www.haskell.org/">Haskell</a> library for working with Hidden Markov Models.</li>
<li><a rel="nofollow" class="external text" href="http://gt2k.cc.gatech.edu/">GT2K</a> Georgia Tech Gesture Toolkit (referred to as GT2K)</li>
<li><a rel="nofollow" class="external text" href="http://www.lwebzem.com/cgi-bin/courses/hidden_markov_model_online.cgi">Hidden Markov Models -online calculator for HMM – Viterbi path and probabilities. Examples with perl source code.</a></li>
<li><a rel="nofollow" class="external text" href="http://sourceforge.net/projects/cvhmm/">A discrete Hidden Markov Model class, based on OpenCV.</a></li>
<li><a rel="nofollow" class="external text" href="http://cran.r-project.org/web/packages/depmixS4/index.html">depmixS4</a> R-Package (Hidden Markov Models of GLMs and Other Distributions in S4 )</li>
<li><a href="https://en.wikipedia.org/wiki/MLPACK_(C%2B%2B_library)" title="MLPACK (C++ library)">MLPACK</a> contains a C++ implementation of HMMs</li>
<li><a rel="nofollow" class="external text" href="http://adrianulbona.github.io/hmm">Hidden Markov Models Java Library</a> contains basic HMMs abstractions in Java 8</li>
</ul>
<table class="navbox" style="border-spacing:0">
<tbody><tr>
<td style="padding:2px">
<table class="nowraplinks collapsible uncollapsed navbox-inner" style="border-spacing:0;background:transparent;color:inherit" id="collapsibleTable0">
<tbody><tr>
<th scope="col" class="navbox-title" colspan="2"><span class="collapseButton">[<a id="collapseButton0" href="https://en.wikipedia.org/wiki/Hidden_Markov_model#">hide</a>]</span>
<div class="plainlinks hlist navbar mini">
<ul>
<li class="nv-view"><a href="https://en.wikipedia.org/wiki/Template:Stochastic_processes" title="Template:Stochastic processes"><abbr title="View this template" style=";;background:none transparent;border:none;">v</abbr></a></li>
<li class="nv-talk"><a href="https://en.wikipedia.org/wiki/Template_talk:Stochastic_processes" title="Template talk:Stochastic processes"><abbr title="Discuss this template" style=";;background:none transparent;border:none;">t</abbr></a></li>
<li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Stochastic_processes&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;">e</abbr></a></li>
</ul>
</div>
<div style="font-size:114%"><a href="https://en.wikipedia.org/wiki/Stochastic_process" title="Stochastic process">Stochastic processes</a></div>
</th>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="https://en.wikipedia.org/wiki/Discrete-time_stochastic_process" title="Discrete-time stochastic process">Discrete time</a></th>
<td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Bernoulli_process" title="Bernoulli process">Bernoulli process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Branching_process" title="Branching process">Branching process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Chinese_restaurant_process" title="Chinese restaurant process">Chinese restaurant process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Galton%E2%80%93Watson_process" title="Galton–Watson process">Galton–Watson process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" title="Independent and identically distributed random variables">Independent and identically distributed random variables</a></li>
<li><a href="https://en.wikipedia.org/wiki/Markov_chain" title="Markov chain">Markov chain</a></li>
<li><a href="https://en.wikipedia.org/wiki/Moran_process" title="Moran process">Moran process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Random_walk" title="Random walk">Random walk</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Loop-erased_random_walk" title="Loop-erased random walk">Loop-erased</a></li>
<li><a href="https://en.wikipedia.org/wiki/Self-avoiding_walk" title="Self-avoiding walk">Self-avoiding</a></li>
</ul>
</li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="https://en.wikipedia.org/wiki/Continuous-time_stochastic_process" title="Continuous-time stochastic process">Continuous time</a></th>
<td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Bessel_process" title="Bessel process">Bessel process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Birth%E2%80%93death_process" title="Birth–death process">Birth–death process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wiener_process" title="Wiener process">Brownian motion</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Brownian_bridge" title="Brownian bridge">Bridge</a></li>
<li><a href="https://en.wikipedia.org/wiki/Brownian_excursion" title="Brownian excursion">Excursion</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fractional_Brownian_motion" title="Fractional Brownian motion">Fractional</a></li>
<li><a href="https://en.wikipedia.org/wiki/Geometric_Brownian_motion" title="Geometric Brownian motion">Geometric</a></li>
<li><a href="https://en.wikipedia.org/wiki/Brownian_meander" title="Brownian meander">Meander</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Cauchy_process" title="Cauchy process">Cauchy process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Contact_process_(mathematics)" title="Contact process (mathematics)">Contact process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Continuous-time_random_walk" title="Continuous-time random walk">Continuous-time random walk</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cox_process" title="Cox process">Cox process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Diffusion_process" title="Diffusion process">Diffusion process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Empirical_process" title="Empirical process">Empirical process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feller_process" title="Feller process">Feller process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fleming%E2%80%93Viot_process" title="Fleming–Viot process">Fleming–Viot process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gamma_process" title="Gamma process">Gamma process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hunt_process" title="Hunt process">Hunt process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Interacting_particle_system" title="Interacting particle system">Interacting particle systems</a></li>
<li><a href="https://en.wikipedia.org/wiki/It%C3%B4_diffusion" title="Itô diffusion">Itô diffusion</a></li>
<li><a href="https://en.wikipedia.org/wiki/It%C3%B4_process" title="Itô process" class="mw-redirect">Itô process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Jump_diffusion" title="Jump diffusion">Jump diffusion</a></li>
<li><a href="https://en.wikipedia.org/wiki/Jump_process" title="Jump process">Jump process</a></li>
<li><a href="https://en.wikipedia.org/wiki/L%C3%A9vy_process" title="Lévy process">Lévy process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Local_time_(mathematics)" title="Local time (mathematics)">Local time</a></li>
<li><a href="https://en.wikipedia.org/wiki/Markov_additive_process" title="Markov additive process">Markov additive process</a></li>
<li><a href="https://en.wikipedia.org/wiki/McKean%E2%80%93Vlasov_process" title="McKean–Vlasov process">McKean–Vlasov process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" title="Ornstein–Uhlenbeck process">Ornstein–Uhlenbeck process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Poisson_process" title="Poisson process" class="mw-redirect">Poisson process</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Compound_Poisson_process" title="Compound Poisson process">Compound</a></li>
<li><a href="https://en.wikipedia.org/wiki/Non-homogeneous_Poisson_process" title="Non-homogeneous Poisson process" class="mw-redirect">Non-homogeneous</a></li>
<li><a href="https://en.wikipedia.org/wiki/Poisson_point_process" title="Poisson point process">Point process</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Schramm%E2%80%93Loewner_evolution" title="Schramm–Loewner evolution">Schramm–Loewner evolution</a></li>
<li><a href="https://en.wikipedia.org/wiki/Semimartingale" title="Semimartingale">Semimartingale</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sigma-martingale" title="Sigma-martingale">Sigma-martingale</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stable_process" title="Stable process">Stable process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Superprocess" title="Superprocess">Superprocess</a></li>
<li><a href="https://en.wikipedia.org/wiki/Telegraph_process" title="Telegraph process">Telegraph process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Variance_gamma_process" title="Variance gamma process">Variance gamma process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wiener_process" title="Wiener process">Wiener process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wiener_sausage" title="Wiener sausage">Wiener sausage</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group">Both</th>
<td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Branching_process" title="Branching process">Branching process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gaussian_process" title="Gaussian process">Gaussian process</a></li>
<li><strong class="selflink">Hidden Markov model (HMM)</strong></li>
<li><a href="https://en.wikipedia.org/wiki/Markov_process" title="Markov process">Markov process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Martingale_(probability_theory)" title="Martingale (probability theory)">Martingale</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Martingale_difference_sequence" title="Martingale difference sequence">Differences</a></li>
<li><a href="https://en.wikipedia.org/wiki/Local_martingale" title="Local martingale">Local</a></li>
<li><a href="https://en.wikipedia.org/wiki/Submartingale" title="Submartingale" class="mw-redirect">Sub-</a></li>
<li><a href="https://en.wikipedia.org/wiki/Supermartingale" title="Supermartingale" class="mw-redirect">Super-</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Random_dynamical_system" title="Random dynamical system">Random dynamical system</a></li>
<li><a href="https://en.wikipedia.org/wiki/Regenerative_process" title="Regenerative process">Regenerative process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Renewal_process" title="Renewal process" class="mw-redirect">Renewal process</a></li>
<li><a href="https://en.wikipedia.org/wiki/White_noise" title="White noise">White noise</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group">Fields and other</th>
<td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Dirichlet_process" title="Dirichlet process">Dirichlet process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gaussian_random_field" title="Gaussian random field">Gaussian random field</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gibbs_measure" title="Gibbs measure">Gibbs measure</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hopfield_model" title="Hopfield model" class="mw-redirect">Hopfield model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ising_model" title="Ising model">Ising model</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Potts_model" title="Potts model">Potts model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Boolean_network" title="Boolean network">Boolean network</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Markov_random_field" title="Markov random field">Markov random field</a></li>
<li><a href="https://en.wikipedia.org/wiki/Percolation_theory" title="Percolation theory">Percolation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Pitman%E2%80%93Yor_process" title="Pitman–Yor process">Pitman–Yor process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Point_process" title="Point process">Point process</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Point_process#Cox_point_process" title="Point process">Cox</a></li>
<li><a href="https://en.wikipedia.org/wiki/Poisson_point_process" title="Poisson point process">Poisson</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Random_field" title="Random field">Random field</a></li>
<li><a href="https://en.wikipedia.org/wiki/Random_graph" title="Random graph">Random graph</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="https://en.wikipedia.org/wiki/Time_series" title="Time series">Time series models</a></th>
<td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity" title="Autoregressive conditional heteroskedasticity">Autoregressive conditional heteroskedasticity (ARCH) model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average" title="Autoregressive integrated moving average">Autoregressive integrated moving average (ARIMA) model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Autoregressive_model" title="Autoregressive model">Autoregressive (AR) model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Autoregressive%E2%80%93moving-average_model" title="Autoregressive–moving-average model">Autoregressive–moving-average (ARMA) model</a></li>
<li><a href="https://en.wikipedia.org/wiki/GARCH" title="GARCH" class="mw-redirect">Generalized autoregressive conditional heteroskedasticity (GARCH) model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Moving-average_model" title="Moving-average model">Moving-average (MA) model</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group">Financial models</th>
<td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Black%E2%80%93Derman%E2%80%93Toy_model" title="Black–Derman–Toy model">Black–Derman–Toy</a></li>
<li><a href="https://en.wikipedia.org/wiki/Black%E2%80%93Karasinski_model" title="Black–Karasinski model">Black–Karasinski</a></li>
<li><a href="https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model" title="Black–Scholes model">Black–Scholes</a></li>
<li><a href="https://en.wikipedia.org/wiki/Chen_model" title="Chen model">Chen</a></li>
<li><a href="https://en.wikipedia.org/wiki/Constant_elasticity_of_variance_model" title="Constant elasticity of variance model">Constant elasticity of variance (CEV)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cox%E2%80%93Ingersoll%E2%80%93Ross_model" title="Cox–Ingersoll–Ross model">Cox–Ingersoll–Ross (CIR)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Garman%E2%80%93Kohlhagen_model" title="Garman–Kohlhagen model" class="mw-redirect">Garman–Kohlhagen</a></li>
<li><a href="https://en.wikipedia.org/wiki/Heath%E2%80%93Jarrow%E2%80%93Morton_framework" title="Heath–Jarrow–Morton framework">Heath–Jarrow–Morton (HJM)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Heston_model" title="Heston model">Heston</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ho%E2%80%93Lee_model" title="Ho–Lee model">Ho–Lee</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hull%E2%80%93White_model" title="Hull–White model">Hull–White</a></li>
<li><a href="https://en.wikipedia.org/wiki/LIBOR_market_model" title="LIBOR market model">LIBOR market</a></li>
<li><a href="https://en.wikipedia.org/wiki/Rendleman%E2%80%93Bartter_model" title="Rendleman–Bartter model">Rendleman–Bartter</a></li>
<li><a href="https://en.wikipedia.org/wiki/SABR_volatility_model" title="SABR volatility model">SABR volatility</a></li>
<li><a href="https://en.wikipedia.org/wiki/Vasicek_model" title="Vasicek model">Vašíček</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wilkie_investment_model" title="Wilkie investment model">Wilkie</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="https://en.wikipedia.org/wiki/Actuarial_mathematics" title="Actuarial mathematics" class="mw-redirect">Actuarial models</a></th>
<td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/B%C3%BChlmann_model" title="Bühlmann model">Bühlmann</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Lundberg_model" title="Cramér–Lundberg model" class="mw-redirect">Cramér–Lundberg</a></li>
<li><a href="https://en.wikipedia.org/wiki/Risk_process" title="Risk process" class="mw-redirect">Risk process</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sparre%E2%80%93Anderson_model" title="Sparre–Anderson model" class="mw-redirect">Sparre–Anderson</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="https://en.wikipedia.org/wiki/Queueing_model" title="Queueing model" class="mw-redirect">Queueing models</a></th>
<td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Bulk_queue" title="Bulk queue">Bulk</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fluid_queue" title="Fluid queue">Fluid</a></li>
<li><a href="https://en.wikipedia.org/wiki/G-network" title="G-network">Generalized queueing network</a></li>
<li><a href="https://en.wikipedia.org/wiki/M/G/1_queue" title="M/G/1 queue">M/G/1</a></li>
<li><a href="https://en.wikipedia.org/wiki/M/M/1_queue" title="M/M/1 queue">M/M/1</a></li>
<li><a href="https://en.wikipedia.org/wiki/M/M/c_queue" title="M/M/c queue">M/M/c</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group">Properties</th>
<td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/C%C3%A0dl%C3%A0g" title="Càdlàg">Càdlàg paths</a></li>
<li><a href="https://en.wikipedia.org/wiki/Continuous_stochastic_process" title="Continuous stochastic process">Continuous</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sample-continuous_process" title="Sample-continuous process">Continuous paths</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ergodicity" title="Ergodicity">Ergodic</a></li>
<li><a href="https://en.wikipedia.org/wiki/Exchangeable_random_variables" title="Exchangeable random variables">Exchangeable</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feller-continuous_process" title="Feller-continuous process">Feller-continuous</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_process" title="Gauss–Markov process">Gauss–Markov</a></li>
<li><a href="https://en.wikipedia.org/wiki/Markov_property" title="Markov property">Markov</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mixing_(mathematics)" title="Mixing (mathematics)">Mixing</a></li>
<li><a href="https://en.wikipedia.org/wiki/Piecewise_deterministic_Markov_process" title="Piecewise deterministic Markov process" class="mw-redirect">Piecewise deterministic</a></li>
<li><a href="https://en.wikipedia.org/wiki/Predictable_process" title="Predictable process">Predictable</a></li>
<li><a href="https://en.wikipedia.org/wiki/Progressively_measurable_process" title="Progressively measurable process">Progressively measurable</a></li>
<li><a href="https://en.wikipedia.org/wiki/Self-similar_process" title="Self-similar process">Self-similar</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stationary_process" title="Stationary process">Stationary</a></li>
<li><a href="https://en.wikipedia.org/wiki/Time_reversibility" title="Time reversibility">Time-reversible</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group">Limit theorems</th>
<td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Central_limit_theorem" title="Central limit theorem">Central limit theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Donsker%27s_theorem" title="Donsker&#39;s theorem">Donsker's theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Doob%27s_martingale_convergence_theorems" title="Doob&#39;s martingale convergence theorems">Doob's martingale convergence theorems</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ergodic_theorem" title="Ergodic theorem" class="mw-redirect">Ergodic theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fisher%E2%80%93Tippett%E2%80%93Gnedenko_theorem" title="Fisher–Tippett–Gnedenko theorem">Fisher–Tippett–Gnedenko theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Large_deviation_principle" title="Large deviation principle" class="mw-redirect">Large deviation principle</a></li>
<li><a href="https://en.wikipedia.org/wiki/Law_of_large_numbers" title="Law of large numbers">Law of large numbers (weak/strong)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm" title="Law of the iterated logarithm">Law of the iterated logarithm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Maximal_ergodic_theorem" title="Maximal ergodic theorem">Maximal ergodic theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sanov%27s_theorem" title="Sanov&#39;s theorem">Sanov's theorem</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group"><a href="https://en.wikipedia.org/wiki/List_of_inequalities#Probability_theory_and_statistics" title="List of inequalities">Inequalities</a></th>
<td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Burkholder%E2%80%93Davis%E2%80%93Gundy_inequalities" title="Burkholder–Davis–Gundy inequalities" class="mw-redirect">Burkholder–Davis–Gundy</a></li>
<li><a href="https://en.wikipedia.org/wiki/Doob%27s_martingale_inequality" title="Doob&#39;s martingale inequality">Doob's martingale</a></li>
<li><a href="https://en.wikipedia.org/wiki/Kunita%E2%80%93Watanabe_inequality" title="Kunita–Watanabe inequality" class="mw-redirect">Kunita–Watanabe</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group">Tools</th>
<td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Cameron%E2%80%93Martin_formula" title="Cameron–Martin formula" class="mw-redirect">Cameron–Martin formula</a></li>
<li><a href="https://en.wikipedia.org/wiki/Convergence_of_random_variables" title="Convergence of random variables">Convergence of random variables</a></li>
<li><a href="https://en.wikipedia.org/wiki/Dol%C3%A9ans-Dade_exponential" title="Doléans-Dade exponential">Doléans-Dade exponential</a></li>
<li><a href="https://en.wikipedia.org/wiki/Doob_decomposition_theorem" title="Doob decomposition theorem">Doob decomposition theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Doob%E2%80%93Meyer_decomposition_theorem" title="Doob–Meyer decomposition theorem">Doob–Meyer decomposition theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Doob%27s_optional_stopping_theorem" title="Doob&#39;s optional stopping theorem" class="mw-redirect">Doob's optional stopping theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Dynkin%27s_formula" title="Dynkin&#39;s formula">Dynkin's formula</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feynman%E2%80%93Kac_formula" title="Feynman–Kac formula">Feynman–Kac formula</a></li>
<li><a href="https://en.wikipedia.org/wiki/Filtration_(mathematics)#Measure_theory" title="Filtration (mathematics)">Filtration</a></li>
<li><a href="https://en.wikipedia.org/wiki/Girsanov_theorem" title="Girsanov theorem">Girsanov theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Infinitesimal_generator_(stochastic_processes)" title="Infinitesimal generator (stochastic processes)">Infinitesimal generator</a></li>
<li><a href="https://en.wikipedia.org/wiki/It%C3%B4_integral" title="Itô integral" class="mw-redirect">Itô integral</a></li>
<li><a href="https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma" title="Itô&#39;s lemma">Itô's lemma</a></li>
<li><a href="https://en.wikipedia.org/wiki/Kolmogorov_continuity_theorem" title="Kolmogorov continuity theorem">Kolmogorov continuity theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem" title="Kolmogorov extension theorem">Kolmogorov extension theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/L%C3%A9vy%E2%80%93Prokhorov_metric" title="Lévy–Prokhorov metric">Lévy–Prokhorov metric</a></li>
<li><a href="https://en.wikipedia.org/wiki/Malliavin_calculus" title="Malliavin calculus">Malliavin calculus</a></li>
<li><a href="https://en.wikipedia.org/wiki/Martingale_representation_theorem" title="Martingale representation theorem">Martingale representation theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Optional_stopping_theorem" title="Optional stopping theorem">Optional stopping theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Prokhorov%27s_theorem" title="Prokhorov&#39;s theorem">Prokhorov's theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Quadratic_variation" title="Quadratic variation">Quadratic variation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Reflection_principle_(Wiener_process)" title="Reflection principle (Wiener process)">Reflection principle</a></li>
<li><a href="https://en.wikipedia.org/wiki/Skorokhod_integral" title="Skorokhod integral">Skorokhod integral</a></li>
<li><a href="https://en.wikipedia.org/wiki/Skorokhod%27s_representation_theorem" title="Skorokhod&#39;s representation theorem">Skorokhod's representation theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Skorokhod_space" title="Skorokhod space" class="mw-redirect">Skorokhod space</a></li>
<li><a href="https://en.wikipedia.org/wiki/Snell_envelope" title="Snell envelope">Snell envelope</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation" title="Stochastic differential equation">Stochastic differential equation</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Tanaka_equation" title="Tanaka equation">Tanaka</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Stopping_time" title="Stopping time">Stopping time</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stratonovich_integral" title="Stratonovich integral">Stratonovich integral</a></li>
<li><a href="https://en.wikipedia.org/wiki/Uniform_integrability" title="Uniform integrability">Uniform integrability</a></li>
<li><a href="https://en.wikipedia.org/wiki/Usual_hypotheses" title="Usual hypotheses">Usual hypotheses</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wiener_space" title="Wiener space" class="mw-redirect">Wiener space</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Classical_Wiener_space" title="Classical Wiener space">Classical</a></li>
<li><a href="https://en.wikipedia.org/wiki/Abstract_Wiener_space" title="Abstract Wiener space">Abstract</a></li>
</ul>
</li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<th scope="row" class="navbox-group">Disciplines</th>
<td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Actuarial_mathematics" title="Actuarial mathematics" class="mw-redirect">Actuarial mathematics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Econometrics" title="Econometrics">Econometrics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ergodic_theory" title="Ergodic theory">Ergodic theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Extreme_value_theory" title="Extreme value theory">Extreme value theory (EVT)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Large_deviations_theory" title="Large deviations theory">Large deviations theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mathematical_finance" title="Mathematical finance">Mathematical finance</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mathematical_statistics" title="Mathematical statistics">Mathematical statistics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Probability_theory" title="Probability theory">Probability theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Queueing_theory" title="Queueing theory">Queueing theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Renewal_theory" title="Renewal theory">Renewal theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ruin_theory" title="Ruin theory">Ruin theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Statistics" title="Statistics">Statistics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stochastic_analysis" title="Stochastic analysis" class="mw-redirect">Stochastic analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Time_series_analysis" title="Time series analysis" class="mw-redirect">Time series analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine learning">Machine learning</a></li>
</ul>
</div>
</td>
</tr>
<tr style="height:2px">
<td colspan="2"></td>
</tr>
<tr>
<td class="navbox-abovebelow hlist" colspan="2">
<div>
<ul>
<li><a href="https://en.wikipedia.org/wiki/List_of_stochastic_processes_topics" title="List of stochastic processes topics">List of topics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Category:Stochastic_processes" title="Category:Stochastic processes">Category</a></li>
</ul>
</div>
</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody></table>
<table class="navbox" style="border-spacing:0">
<tbody><tr>
<td style="padding:2px">
<table class="nowraplinks hlist navbox-inner" style="border-spacing:0;background:transparent;color:inherit">
<tbody><tr>
<th scope="row" class="navbox-group"><a href="https://en.wikipedia.org/wiki/Help:Authority_control" title="Help:Authority control">Authority control</a></th>
<td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Integrated_Authority_File" title="Integrated Authority File">GND</a>: <span class="uid"><a rel="nofollow" class="external text" href="http://d-nb.info/gnd/4352479-5">4352479-5</a></span></li>
</ul>
</div>
</td>
</tr>
</tbody></table>
</td>
</tr>
</tbody></table>


<!-- 
NewPP limit report
Parsed by mw1245
Cached time: 20160509115017
Cache expiry: 2592000
Dynamic content: false
CPU time usage: 0.413 seconds
Real time usage: 0.694 seconds
Preprocessor visited node count: 3711/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 100784/2097152 bytes
Template argument size: 1529/2097152 bytes
Highest expansion depth: 12/40
Expensive parser function count: 0/500
Lua time usage: 0.113/10.000 seconds
Lua memory usage: 3.47 MB/50 MB
Number of Wikibase entities loaded: 1-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%  558.571      1 - -total
 24.21%  135.228      1 - Template:Reflist
 16.67%   93.132     16 - Template:Cite_journal
  8.64%   48.268      1 - Template:Machine_learning_bar
  8.19%   45.760      1 - Template:Sidebar_with_collapsible_lists
  3.77%   21.031      1 - Template:Commons_category_inline
  3.08%   17.225      1 - Template:Stochastic_processes
  2.94%   16.420      1 - Template:Sister-inline
  2.63%   14.675      1 - Template:Authority_control
  2.57%   14.374      1 - Template:Navbox
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:98770-0!*!0!!en!4!*!math=0 and timestamp 20160509115017 and revision id 716209275
 -->
<noscript>&lt;img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /&gt;</noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;oldid=716209275">https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;oldid=716209275</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="https://en.wikipedia.org/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Bioinformatics" title="Category:Bioinformatics">Bioinformatics</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Hidden_Markov_models" title="Category:Hidden Markov models">Hidden Markov models</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Markov_models" title="Category:Markov models">Markov models</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Commons_category_with_local_link_same_as_on_Wikidata" title="Category:Commons category with local link same as on Wikidata">Commons category with local link same as on Wikidata</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_with_GND_identifiers" title="Category:Wikipedia articles with GND identifiers">Wikipedia articles with GND identifiers</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_example_Python_code" title="Category:Articles with example Python code">Articles with example Python code</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>

			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="https://en.wikipedia.org/wiki/Special:MyTalk" title="Discussion about edits from this IP address [alt-shift-n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="https://en.wikipedia.org/wiki/Special:MyContributions" title="A list of edits made from this IP address [alt-shift-y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="https://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=Hidden+Markov+model&amp;type=signup" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="https://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=Hidden+Markov+model" title="You&#39;re encouraged to log in; however, it&#39;s not mandatory. [alt-shift-o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
															<li id="ca-nstab-main" class="selected"><span><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" title="View the content page [alt-shift-c]" accesskey="c">Article</a></span></li>
															<li id="ca-talk"><span><a href="https://en.wikipedia.org/wiki/Talk:Hidden_Markov_model" title="Discussion about the content page [alt-shift-t]" accesskey="t" rel="discussion">Talk</a></span></li>
													</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<h3 id="p-variants-label" tabindex="0">
							<span>Variants</span><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#" tabindex="-1"></a>
						</h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
															<li id="ca-view" class="selected"><span><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model">Read</a></span></li>
															<li id="ca-edit"><span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=edit" title="Edit this page [alt-shift-e]" accesskey="e">Edit</a></span></li>
															<li id="ca-history" class="collapsible"><span><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=history" title="Past revisions of this page [alt-shift-h]" accesskey="h">View history</a></span></li>
													</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<h3 id="p-cactions-label" tabindex="0"><span>More</span><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model#" tabindex="-1"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>

						<form action="https://en.wikipedia.org/w/index.php" id="searchform">
							<div id="simpleSearch">
							<input type="search" name="search" placeholder="Search" title="Search Wikipedia [alt-shift-f]" accesskey="f" id="searchInput" tabindex="1" autocomplete="off"><input type="hidden" value="Special:Search" name="title"><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton">							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>

			<div class="body">
									<ul>
						<li id="n-mainpage-description"><a href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page [alt-shift-z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="https://en.wikipedia.org/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="https://en.wikipedia.org/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="https://en.wikipedia.org/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="https://en.wikipedia.org/wiki/Special:Random" title="Load a random article [alt-shift-x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="https://shop.wikimedia.org/" title="Visit the Wikipedia store">Wikipedia store</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>

			<div class="body">
									<ul>
						<li id="n-help"><a href="https://en.wikipedia.org/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="https://en.wikipedia.org/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="https://en.wikipedia.org/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [alt-shift-r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>

			<div class="body">
									<ul>
						<li id="t-whatlinkshere"><a href="https://en.wikipedia.org/wiki/Special:WhatLinksHere/Hidden_Markov_model" title="List of all English Wikipedia pages containing links to this page [alt-shift-j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Hidden_Markov_model" title="Recent changes in pages linked from this page [alt-shift-k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [alt-shift-u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="https://en.wikipedia.org/wiki/Special:SpecialPages" title="A list of all special pages [alt-shift-q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;oldid=716209275" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Q176769" title="Link to connected data repository item [alt-shift-g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=Hidden_Markov_model&amp;id=716209275" title="Information on how to cite this page">Cite this page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>

			<div class="body">
									<ul>
						<li id="coll-create_a_book"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Hidden+Markov+model">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=render_article&amp;arttitle=Hidden+Markov+model&amp;returnto=Hidden+Markov+model&amp;oldid=716209275&amp;writer=rdf2latex">Download as PDF</a></li><li id="t-print"><a href="https://en.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;printable=yes" title="Printable version of this page [alt-shift-p]" accesskey="p">Printable version</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-wikibase-otherprojects" aria-labelledby="p-wikibase-otherprojects-label">
			<h3 id="p-wikibase-otherprojects-label">In other projects</h3>

			<div class="body">
									<ul>
						<li class="wb-otherproject-link wb-otherproject-commons"><a href="https://commons.wikimedia.org/wiki/Category:Hidden_Markov_Model" hreflang="en">Wikimedia Commons</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label"><span class="uls-settings-trigger" title="Language settings" tabindex="0" role="button" aria-haspopup="true"></span>
			<h3 id="p-lang-label">Languages</h3>

			<div class="body">
									<ul>
						<li class="interlanguage-link interwiki-af"><a href="https://af.wikipedia.org/wiki/Verborge_Markovmodel" title="Verborge Markovmodel – Afrikaans" lang="af" hreflang="af">Afrikaans</a></li><li class="interlanguage-link interwiki-bg"><a href="https://bg.wikipedia.org/wiki/%D0%A1%D0%BA%D1%80%D0%B8%D1%82_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB_%D0%BD%D0%B0_%D0%9C%D0%B0%D1%80%D0%BA%D0%BE%D0%B2" title="Скрит модел на Марков – Bulgarian" lang="bg" hreflang="bg">Български</a></li><li class="interlanguage-link interwiki-ca"><a href="https://ca.wikipedia.org/wiki/Model_ocult_de_Markov" title="Model ocult de Markov – Catalan" lang="ca" hreflang="ca">Català</a></li><li class="interlanguage-link interwiki-cs"><a href="https://cs.wikipedia.org/wiki/Skryt%C3%BD_Markov%C5%AFv_model" title="Skrytý Markovův model – Czech" lang="cs" hreflang="cs">Čeština</a></li><li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Hidden_Markov_Model" title="Hidden Markov Model – German" lang="de" hreflang="de">Deutsch</a></li><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Modelo_oculto_de_M%C3%A1rkov" title="Modelo oculto de Márkov – Spanish" lang="es" hreflang="es">Español</a></li><li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%D9%85%D8%AF%D9%84_%D9%BE%D9%86%D9%87%D8%A7%D9%86_%D9%85%D8%A7%D8%B1%DA%A9%D9%81" title="مدل پنهان مارکف – Persian" lang="fa" hreflang="fa">فارسی</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Mod%C3%A8le_de_Markov_cach%C3%A9" title="Modèle de Markov caché – French" lang="fr" hreflang="fr">Français</a></li><li class="interlanguage-link interwiki-ko"><a href="https://ko.wikipedia.org/wiki/%EC%9D%80%EB%8B%89_%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EB%AA%A8%EB%8D%B8" title="은닉 마르코프 모델 – Korean" lang="ko" hreflang="ko">한국어</a></li><li class="interlanguage-link interwiki-id"><a href="https://id.wikipedia.org/wiki/Model_Markov_tersembunyi" title="Model Markov tersembunyi – Indonesian" lang="id" hreflang="id">Bahasa Indonesia</a></li><li class="interlanguage-link interwiki-it"><a href="https://it.wikipedia.org/wiki/Modello_di_Markov_nascosto" title="Modello di Markov nascosto – Italian" lang="it" hreflang="it">Italiano</a></li><li class="interlanguage-link interwiki-he"><a href="https://he.wikipedia.org/wiki/%D7%9E%D7%95%D7%93%D7%9C_%D7%9E%D7%A8%D7%A7%D7%95%D7%91_%D7%97%D7%91%D7%95%D7%99" title="מודל מרקוב חבוי – Hebrew" lang="he" hreflang="he">עברית</a></li><li class="interlanguage-link interwiki-nl"><a href="https://nl.wikipedia.org/wiki/Hidden_Markov_model" title="Hidden Markov model – Dutch" lang="nl" hreflang="nl">Nederlands</a></li><li class="interlanguage-link interwiki-ja"><a href="https://ja.wikipedia.org/wiki/%E9%9A%A0%E3%82%8C%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E3%83%A2%E3%83%87%E3%83%AB" title="隠れマルコフモデル – Japanese" lang="ja" hreflang="ja">日本語</a></li><li class="interlanguage-link interwiki-pt"><a href="https://pt.wikipedia.org/wiki/Modelo_oculto_de_Markov" title="Modelo oculto de Markov – Portuguese" lang="pt" hreflang="pt">Português</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/%D0%A1%D0%BA%D1%80%D1%8B%D1%82%D0%B0%D1%8F_%D0%BC%D0%B0%D1%80%D0%BA%D0%BE%D0%B2%D1%81%D0%BA%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C" title="Скрытая марковская модель – Russian" lang="ru" hreflang="ru">Русский</a></li><li class="interlanguage-link interwiki-sl"><a href="https://sl.wikipedia.org/wiki/Skriti_model_Markova" title="Skriti model Markova – Slovenian" lang="sl" hreflang="sl">Slovenščina</a></li><li class="interlanguage-link interwiki-sr"><a href="https://sr.wikipedia.org/wiki/Skriveni_Markovljev_model" title="Skriveni Markovljev model – Serbian" lang="sr" hreflang="sr">Српски / srpski</a></li><li class="interlanguage-link interwiki-sh"><a href="https://sh.wikipedia.org/wiki/Skriveni_Markovljev_model" title="Skriveni Markovljev model – Serbo-Croatian" lang="sh" hreflang="sh">Srpskohrvatski / српскохрватски</a></li><li class="interlanguage-link interwiki-fi"><a href="https://fi.wikipedia.org/wiki/Markovin_piilomalli" title="Markovin piilomalli – Finnish" lang="fi" hreflang="fi">Suomi</a></li><li class="interlanguage-link interwiki-sv"><a href="https://sv.wikipedia.org/wiki/Dold_Markovmodell" title="Dold Markovmodell – Swedish" lang="sv" hreflang="sv">Svenska</a></li><li class="interlanguage-link interwiki-th"><a href="https://th.wikipedia.org/wiki/%E0%B9%81%E0%B8%9A%E0%B8%9A%E0%B8%88%E0%B8%B3%E0%B8%A5%E0%B8%AD%E0%B8%87%E0%B8%A1%E0%B8%B2%E0%B8%A3%E0%B9%8C%E0%B8%84%E0%B8%AD%E0%B8%9F%E0%B8%8B%E0%B9%88%E0%B8%AD%E0%B8%99%E0%B9%80%E0%B8%A3%E0%B9%89%E0%B8%99" title="แบบจำลองมาร์คอฟซ่อนเร้น – Thai" lang="th" hreflang="th">ไทย</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%9F%D1%80%D0%B8%D1%85%D0%BE%D0%B2%D0%B0%D0%BD%D0%B0_%D0%BC%D0%B0%D1%80%D0%BA%D0%BE%D0%B2%D1%81%D1%8C%D0%BA%D0%B0_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C" title="Прихована марковська модель – Ukrainian" lang="uk" hreflang="uk">Українська</a></li><li class="interlanguage-link interwiki-vi"><a href="https://vi.wikipedia.org/wiki/M%C3%B4_h%C3%ACnh_Markov_%E1%BA%A9n" title="Mô hình Markov ẩn – Vietnamese" lang="vi" hreflang="vi">Tiếng Việt</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B" title="隐马尔可夫模型 – Chinese" lang="zh" hreflang="zh">中文</a></li>					</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Q176769#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last modified on 20 April 2016, at 15:33.</li>
											<li id="footer-info-copyright">Text is available under the <a rel="license" href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="https://wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="https://wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="https://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
									</ul>
							<ul id="footer-places">
											<li id="footer-places-privacy"><a href="https://wikimediafoundation.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
											<li id="footer-places-about"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
											<li id="footer-places-disclaimer"><a href="https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
											<li id="footer-places-contact"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
											<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
											<li id="footer-places-cookiestatement"><a href="https://wikimediafoundation.org/wiki/Cookie_statement">Cookie statement</a></li>
											<li id="footer-places-mobileview"><a href="https://en.m.wikipedia.org/w/index.php?title=Hidden_Markov_model&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
									</ul>
										<ul id="footer-icons" class="noprint">
											<li id="footer-copyrightico">
							<a href="https://wikimediafoundation.org/"><img src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"></a>						</li>
											<li id="footer-poweredbyico">
							<a href="https://www.mediawiki.org/"><img src="./Hidden Markov model - Wikipedia, the free encyclopedia_files/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/w/resources/assets/poweredby_mediawiki_132x47.png 1.5x, /w/resources/assets/poweredby_mediawiki_176x62.png 2x" width="88" height="31"></a>						</li>
									</ul>
						<div style="clear:both"></div>
		</div>
		<script>(window.RLQ=window.RLQ||[]).push(function(){mw.loader.state({"ext.globalCssJs.site":"ready","ext.globalCssJs.user":"ready","user":"ready","user.groups":"ready"});mw.loader.load(["ext.cite.a11y","mediawiki.toc","mediawiki.action.view.postEdit","site","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.searchSuggest","ext.eventLogging.subscriber","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.switcher","ext.gadget.featured-articles-links","mmv.bootstrap.autostart","ext.visualEditor.targetLoader","ext.wikimediaEvents","ext.navigationTiming","schema.UniversalLanguageSelector","ext.uls.eventlogger","ext.uls.interlanguage"]);});</script><script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":66,"wgHostname":"mw1257"});});</script>
	

<div class="suggestions" style="display: none; font-size: 13px;"><div class="suggestions-results"></div><div class="suggestions-special"></div></div></body></html>